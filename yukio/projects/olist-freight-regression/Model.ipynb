{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed7adfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from category_encoders import CatBoostEncoder\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import optuna as opt\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdef6767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>freight_value</th>\n",
       "      <th>product_category_name</th>\n",
       "      <th>product_weight_g</th>\n",
       "      <th>product_length_cm</th>\n",
       "      <th>product_height_cm</th>\n",
       "      <th>product_width_cm</th>\n",
       "      <th>customer_city</th>\n",
       "      <th>customer_state</th>\n",
       "      <th>review_score</th>\n",
       "      <th>...</th>\n",
       "      <th>volume</th>\n",
       "      <th>density</th>\n",
       "      <th>actual_delivery_time</th>\n",
       "      <th>estimated_delivery_time</th>\n",
       "      <th>approval_order_time</th>\n",
       "      <th>distance</th>\n",
       "      <th>purchase_month</th>\n",
       "      <th>purchase_day_of_week</th>\n",
       "      <th>black_friday</th>\n",
       "      <th>christmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>utilidades_domesticas</td>\n",
       "      <td>500.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>sao paulo</td>\n",
       "      <td>SP</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>0.253036</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.566632</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118.70</td>\n",
       "      <td>22.76</td>\n",
       "      <td>perfumaria</td>\n",
       "      <td>400.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>barreiras</td>\n",
       "      <td>BA</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>4693.0</td>\n",
       "      <td>0.085233</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>847.437333</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>159.90</td>\n",
       "      <td>19.22</td>\n",
       "      <td>automotivo</td>\n",
       "      <td>420.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>vianopolis</td>\n",
       "      <td>GO</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>9576.0</td>\n",
       "      <td>0.043860</td>\n",
       "      <td>9.0</td>\n",
       "      <td>26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>512.100044</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.00</td>\n",
       "      <td>27.20</td>\n",
       "      <td>pet_shop</td>\n",
       "      <td>450.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>sao goncalo do amarante</td>\n",
       "      <td>RN</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1816.085655</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.90</td>\n",
       "      <td>8.72</td>\n",
       "      <td>papelaria</td>\n",
       "      <td>250.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>santo andre</td>\n",
       "      <td>SP</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>11475.0</td>\n",
       "      <td>0.021786</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.684401</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    price  freight_value  product_category_name  product_weight_g  \\\n",
       "0   29.99           8.72  utilidades_domesticas             500.0   \n",
       "1  118.70          22.76             perfumaria             400.0   \n",
       "2  159.90          19.22             automotivo             420.0   \n",
       "3   45.00          27.20               pet_shop             450.0   \n",
       "4   19.90           8.72              papelaria             250.0   \n",
       "\n",
       "   product_length_cm  product_height_cm  product_width_cm  \\\n",
       "0               19.0                8.0              13.0   \n",
       "1               19.0               13.0              19.0   \n",
       "2               24.0               19.0              21.0   \n",
       "3               30.0               10.0              20.0   \n",
       "4               51.0               15.0              15.0   \n",
       "\n",
       "             customer_city customer_state  review_score  ...   volume  \\\n",
       "0                sao paulo             SP             4  ...   1976.0   \n",
       "1                barreiras             BA             4  ...   4693.0   \n",
       "2               vianopolis             GO             5  ...   9576.0   \n",
       "3  sao goncalo do amarante             RN             5  ...   6000.0   \n",
       "4              santo andre             SP             5  ...  11475.0   \n",
       "\n",
       "    density  actual_delivery_time  estimated_delivery_time  \\\n",
       "0  0.253036                   8.0                       15   \n",
       "1  0.085233                  13.0                       19   \n",
       "2  0.043860                   9.0                       26   \n",
       "3  0.075000                  13.0                       26   \n",
       "4  0.021786                   2.0                       12   \n",
       "\n",
       "   approval_order_time     distance  purchase_month  purchase_day_of_week  \\\n",
       "0                  0.0    18.566632              10                     0   \n",
       "1                  1.0   847.437333               7                     1   \n",
       "2                  0.0   512.100044               8                     2   \n",
       "3                  0.0  1816.085655              11                     5   \n",
       "4                  0.0    29.684401               2                     1   \n",
       "\n",
       "   black_friday  christmas  \n",
       "0             0          0  \n",
       "1             0          0  \n",
       "2             0          0  \n",
       "3             0          0  \n",
       "4             0          0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the base directory for the data files\n",
    "data_dir = 'data'\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(os.path.join(data_dir, 'df_final.csv'))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aecfa2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting in X and Y\n",
    "X = df.drop(columns='freight_value', axis=1)\n",
    "y = df.freight_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b7ee63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting in train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "616a435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing models\n",
    "model_XGBoost = XGBRegressor(n_estimators = 1000, max_depth = 8, learning_rate = 1e-3, random_state = 0)\n",
    "model_LightGBM = LGBMRegressor(n_estimators = 1000, max_depth = 8, num_leaves = 2^8, learning_rate = 1e-3, n_jobs = -1, verbose = -1, random_state = 0)\n",
    "model_Catboost = CatBoostRegressor(n_estimators = 1000, max_depth = 8, learning_rate = 1e-3, random_state = 0, verbose = 0)\n",
    "model_DecisionTree = DecisionTreeRegressor(random_state = 0, max_depth = 8, min_samples_split = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1693328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "\n",
    "encoder = CatBoostEncoder()\n",
    "X_train_encoded = X_train.copy()\n",
    "X_test_encoded = X_test.copy()\n",
    "\n",
    "for col in X_train_encoded.select_dtypes(include=['object']).columns:\n",
    "    X_train_encoded[col] = encoder.fit_transform(X_train_encoded[col], y_train)\n",
    "    X_test_encoded[col] = encoder.transform(X_test_encoded[col])\n",
    "\n",
    "model_XGBoost.fit(X_train_encoded, y_train)\n",
    "r = permutation_importance(model_XGBoost, X_test_encoded, y_test, n_repeats=30, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66add077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>distance</td>\n",
       "      <td>0.211973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>volume</td>\n",
       "      <td>0.198656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>product_weight_g</td>\n",
       "      <td>0.178174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>price</td>\n",
       "      <td>0.080163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>customer_state</td>\n",
       "      <td>0.025644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>product_length_cm</td>\n",
       "      <td>0.022553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>seller_city</td>\n",
       "      <td>0.021714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>seller_state</td>\n",
       "      <td>0.013614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>product_width_cm</td>\n",
       "      <td>0.004467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>density</td>\n",
       "      <td>0.003321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>purchase_month</td>\n",
       "      <td>0.002923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product_category_name</td>\n",
       "      <td>0.001475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>customer_city</td>\n",
       "      <td>0.001295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>product_height_cm</td>\n",
       "      <td>0.001211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>actual_delivery_time</td>\n",
       "      <td>0.000642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>review_score</td>\n",
       "      <td>0.000312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>black_friday</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>christmas</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>purchase_day_of_week</td>\n",
       "      <td>-0.000202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>approval_order_time</td>\n",
       "      <td>-0.000902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>estimated_delivery_time</td>\n",
       "      <td>-0.001062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Feature  importance\n",
       "16                 distance    0.211973\n",
       "11                   volume    0.198656\n",
       "2          product_weight_g    0.178174\n",
       "0                     price    0.080163\n",
       "7            customer_state    0.025644\n",
       "3         product_length_cm    0.022553\n",
       "9               seller_city    0.021714\n",
       "10             seller_state    0.013614\n",
       "5          product_width_cm    0.004467\n",
       "12                  density    0.003321\n",
       "17           purchase_month    0.002923\n",
       "1     product_category_name    0.001475\n",
       "6             customer_city    0.001295\n",
       "4         product_height_cm    0.001211\n",
       "13     actual_delivery_time    0.000642\n",
       "8              review_score    0.000312\n",
       "19             black_friday    0.000003\n",
       "20                christmas    0.000000\n",
       "18     purchase_day_of_week   -0.000202\n",
       "15      approval_order_time   -0.000902\n",
       "14  estimated_delivery_time   -0.001062"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances = pd.DataFrame({'Feature': X_test_encoded.columns, 'importance': r.importances_mean})\n",
    "importances = importances.sort_values(by='importance', ascending=False)\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bceda8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "less_important_columns = ['purchase_day_of_week', 'approval_order_time', 'estimated_delivery_time', 'christmas', 'black_friday']\n",
    "\n",
    "X_train = X_train.drop(columns=less_important_columns)\n",
    "X_test = X_test.drop(columns=less_important_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d135ac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "\n",
    "encoder = CatBoostEncoder()\n",
    "X_train_encoded = X_train.copy()\n",
    "X_test_encoded = X_test.copy()\n",
    "\n",
    "for col in X_train_encoded.select_dtypes(include=['object']).columns:\n",
    "    X_train_encoded[col] = encoder.fit_transform(X_train_encoded[col], y_train)\n",
    "    X_test_encoded[col] = encoder.transform(X_test_encoded[col])\n",
    "\n",
    "model_XGBoost.fit(X_train_encoded, y_train)\n",
    "r_v2 = permutation_importance(model_XGBoost, X_test_encoded, y_test, n_repeats=30, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a41b22ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>distance</td>\n",
       "      <td>0.211323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>volume</td>\n",
       "      <td>0.203850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>product_weight_g</td>\n",
       "      <td>0.182162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>price</td>\n",
       "      <td>0.090672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>customer_state</td>\n",
       "      <td>0.024785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>product_length_cm</td>\n",
       "      <td>0.024383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>seller_city</td>\n",
       "      <td>0.022433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>seller_state</td>\n",
       "      <td>0.012966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>product_width_cm</td>\n",
       "      <td>0.004509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>customer_city</td>\n",
       "      <td>0.003811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>purchase_month</td>\n",
       "      <td>0.002410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>density</td>\n",
       "      <td>0.001630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product_category_name</td>\n",
       "      <td>0.001497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>product_height_cm</td>\n",
       "      <td>0.001340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>actual_delivery_time</td>\n",
       "      <td>0.000855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>review_score</td>\n",
       "      <td>0.000339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Feature  importance\n",
       "14               distance    0.211323\n",
       "11                 volume    0.203850\n",
       "2        product_weight_g    0.182162\n",
       "0                   price    0.090672\n",
       "7          customer_state    0.024785\n",
       "3       product_length_cm    0.024383\n",
       "9             seller_city    0.022433\n",
       "10           seller_state    0.012966\n",
       "5        product_width_cm    0.004509\n",
       "6           customer_city    0.003811\n",
       "15         purchase_month    0.002410\n",
       "12                density    0.001630\n",
       "1   product_category_name    0.001497\n",
       "4       product_height_cm    0.001340\n",
       "13   actual_delivery_time    0.000855\n",
       "8            review_score    0.000339"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances_v2 = pd.DataFrame({'Feature': X_test_encoded.columns, 'importance': r_v2.importances_mean})\n",
    "importances_v2 = importances_v2.sort_values(by='importance', ascending=False)\n",
    "importances_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa76170a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "MAE: 5.044\n",
      "MSE: 98.518\n",
      "R2: 0.600\n",
      "########## Fold: 2 ##########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 5.188\n",
      "MSE: 106.660\n",
      "R2: 0.597\n",
      "########## Fold: 3 ##########\n",
      "MAE: 5.146\n",
      "MSE: 97.921\n",
      "R2: 0.605\n",
      "########## Fold: 4 ##########\n",
      "MAE: 5.078\n",
      "MSE: 112.953\n",
      "R2: 0.584\n",
      "########## Fold: 5 ##########\n",
      "MAE: 4.990\n",
      "MSE: 88.761\n",
      "R2: 0.620\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "\n",
    "folds = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "absolute_errors = list()\n",
    "squared_errors = list()\n",
    "r2 = list()\n",
    "\n",
    "for k, (train_index, test_index) in enumerate(folds.split(X, y)):\n",
    "    \n",
    "    print(\"#\"*10 + f\" Fold: {k+1} \" + \"#\"*10)\n",
    "    \n",
    "    X_train_internal, y_train_internal = X.iloc[train_index, :], y.iloc[train_index]\n",
    "    X_test_internal, y_test_internal = X.iloc[test_index, :], y.iloc[test_index]\n",
    "\n",
    "    encoder = CatBoostEncoder()\n",
    "    \n",
    "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    num_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "    cat_pipeline = Pipeline([('encoder', encoder), ('imputer', cat_imputer)])\n",
    "    num_pipeline = Pipeline([('imputer', num_imputer)])\n",
    "\n",
    "    cat_cols = X_train_internal.select_dtypes(include=['object']).columns\n",
    "    num_cols = X_train_internal.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "    X_train_internal[cat_cols] = cat_pipeline.fit_transform(X_train_internal[cat_cols], y_train_internal)\n",
    "    X_train_internal[num_cols] = num_pipeline.fit_transform(X_train_internal[num_cols])\n",
    "\n",
    "    X_test_internal[cat_cols] = cat_pipeline.transform(X_test_internal[cat_cols])\n",
    "    X_test_internal[num_cols] = num_pipeline.transform(X_test_internal[num_cols])\n",
    "\n",
    "    model_XGBoost.fit(X_train_internal, y_train_internal)\n",
    "    y_pred = model_XGBoost.predict(X_test_internal)\n",
    "    r2score = r2_score(y_test_internal, y_pred)\n",
    "    mse = mean_squared_error(y_test_internal, y_pred)\n",
    "    mae = mean_absolute_error(y_test_internal, y_pred)\n",
    "\n",
    "    absolute_errors.append(mae)\n",
    "    squared_errors.append(mse)\n",
    "    r2.append(r2score)\n",
    "\n",
    "    print(f'MAE: {mae:.3f}')\n",
    "    print(f'MSE: {mse:.3f}')\n",
    "    print(f'R2: {r2score:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86730329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Displaying Average of Obtained Metrics : #####\n",
      "Average MAE: 5.089 +/- 0.071\n",
      "Average MSE: 100.963 +/- 8.250\n",
      "Average R2: 0.601 +/- 0.012\n"
     ]
    }
   ],
   "source": [
    "absolute_errors = np.array(absolute_errors)\n",
    "squared_errors = np.array(squared_errors)\n",
    "r2 = np.array(r2)\n",
    "\n",
    "avg_mae = np.mean(absolute_errors)\n",
    "avg_mse = np.mean(squared_errors)\n",
    "avg_r2 = np.mean(r2)\n",
    "\n",
    "std_mae = np.std(absolute_errors)\n",
    "std_mse = np.std(squared_errors)\n",
    "std_r2 = np.std(r2)\n",
    "\n",
    "print(\"#\"*5 + f\" Displaying Average of Obtained Metrics : \" + \"#\"*5)\n",
    "print(f\"Average MAE: {avg_mae:.3f} +/- {std_mae:.3f}\")\n",
    "print(f'Average MSE: {avg_mse:.3f} +/- {std_mse:.3f}')\n",
    "print(f'Average R2: {avg_r2:.3f} +/- {std_r2:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8b5d1c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(X, y, model, k):\n",
    "    folds = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    absolute_errors = list()\n",
    "    squared_errors = list()\n",
    "    r2 = list()\n",
    "\n",
    "    for k, (train_index, test_index) in enumerate(folds.split(X, y)):\n",
    "        \n",
    "        print(\"#\"*10 + f\" Fold: {k+1} \" + \"#\"*10)\n",
    "        \n",
    "        X_train_internal, y_train_internal = X.iloc[train_index, :], y.iloc[train_index]\n",
    "        X_test_internal, y_test_internal = X.iloc[test_index, :], y.iloc[test_index]\n",
    "\n",
    "        encoder = CatBoostEncoder()\n",
    "        \n",
    "        cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "        num_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "        cat_pipeline = Pipeline([('encoder', encoder), ('imputer', cat_imputer)])\n",
    "        num_pipeline = Pipeline([('imputer', num_imputer)])\n",
    "\n",
    "        cat_cols = X_train_internal.select_dtypes(include=['object']).columns\n",
    "        num_cols = X_train_internal.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "        X_train_internal[cat_cols] = cat_pipeline.fit_transform(X_train_internal[cat_cols], y_train_internal)\n",
    "        X_train_internal[num_cols] = num_pipeline.fit_transform(X_train_internal[num_cols])\n",
    "\n",
    "        X_test_internal[cat_cols] = cat_pipeline.transform(X_test_internal[cat_cols])\n",
    "        X_test_internal[num_cols] = num_pipeline.transform(X_test_internal[num_cols])\n",
    "\n",
    "        model.fit(X_train_internal, y_train_internal)\n",
    "        y_pred = model.predict(X_test_internal)\n",
    "\n",
    "        r2score = r2_score(y_test_internal, y_pred)\n",
    "        mse = mean_squared_error(y_test_internal, y_pred)\n",
    "        mae = mean_absolute_error(y_test_internal, y_pred)\n",
    "\n",
    "        absolute_errors.append(mae)\n",
    "        squared_errors.append(mse)\n",
    "        r2.append(r2score)\n",
    "\n",
    "        print(f'MAE: {mae:.3f}')\n",
    "        print(f'MSE: {mse:.3f}')\n",
    "        print(f'R2: {r2score:.3f}')\n",
    "    \n",
    "    absolute_errors = np.array(absolute_errors)\n",
    "    squared_errors = np.array(squared_errors)\n",
    "    r2 = np.array(r2)\n",
    "\n",
    "    avg_mae = np.mean(absolute_errors)\n",
    "    avg_mse = np.mean(squared_errors)\n",
    "    avg_r2 = np.mean(r2)\n",
    "\n",
    "    std_mae = np.std(absolute_errors)\n",
    "    std_mse = np.std(squared_errors)\n",
    "    std_r2 = np.std(r2)\n",
    "\n",
    "    print(\"#\"*5 + f\" Displaying Average of Obtained Metrics : \" + \"#\"*5)\n",
    "    print(f\"Average MAE: {avg_mae:.3f} +/- {std_mae:.3f}\")\n",
    "    print(f'Average MSE: {avg_mse:.3f} +/- {std_mse:.3f}')\n",
    "    print(f'Average R2: {avg_r2:.3f} +/- {std_r2:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30b1fbc",
   "metadata": {},
   "source": [
    "### Modelo XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "93155869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/16 17:59:01 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'd234fb710552474b817ffa6d0ad4ffd1', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/16 17:59:02 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2025/04/16 17:59:05 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'b9c8dc103a7441d4a688a4d285252c9d', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run stylish-rook-717 at: http://localhost:5000/#/experiments/535394779431182411/runs/d234fb710552474b817ffa6d0ad4ffd1\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/535394779431182411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/16 17:59:05 WARNING mlflow.sklearn: Training metrics will not be recorded because training labels were not specified. To automatically record training metrics, provide training labels as inputs to the model training function.\n",
      "2025/04/16 17:59:05 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run trusting-slug-864 at: http://localhost:5000/#/experiments/535394779431182411/runs/b9c8dc103a7441d4a688a4d285252c9d\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/535394779431182411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/16 17:59:11 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'd354ced7a6404532a49d0bdc3117ed23', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 3.785\n",
      "MSE: 64.887\n",
      "R2: 0.736\n",
      "########## Fold: 2 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/16 17:59:12 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2025/04/16 17:59:15 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '69d6b823955b47bdb3077e54d192b895', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run vaunted-wren-574 at: http://localhost:5000/#/experiments/535394779431182411/runs/d354ced7a6404532a49d0bdc3117ed23\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/535394779431182411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/16 17:59:15 WARNING mlflow.sklearn: Training metrics will not be recorded because training labels were not specified. To automatically record training metrics, provide training labels as inputs to the model training function.\n",
      "2025/04/16 17:59:15 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run skittish-rat-681 at: http://localhost:5000/#/experiments/535394779431182411/runs/69d6b823955b47bdb3077e54d192b895\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/535394779431182411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/16 17:59:22 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '24a8d61a1e6b4643a8c90b0126f3862c', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 3.925\n",
      "MSE: 67.987\n",
      "R2: 0.743\n",
      "########## Fold: 3 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/16 17:59:23 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2025/04/16 17:59:25 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'f74e00750fe64db99a02a5fcbf31f587', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run resilient-robin-357 at: http://localhost:5000/#/experiments/535394779431182411/runs/24a8d61a1e6b4643a8c90b0126f3862c\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/535394779431182411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/16 17:59:26 WARNING mlflow.sklearn: Training metrics will not be recorded because training labels were not specified. To automatically record training metrics, provide training labels as inputs to the model training function.\n",
      "2025/04/16 17:59:26 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run ambitious-donkey-774 at: http://localhost:5000/#/experiments/535394779431182411/runs/f74e00750fe64db99a02a5fcbf31f587\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/535394779431182411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/16 17:59:32 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '4e55bae30f3349ae86fcdce3e1b0a7fd', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 3.843\n",
      "MSE: 62.634\n",
      "R2: 0.747\n",
      "########## Fold: 4 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/16 17:59:33 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2025/04/16 17:59:36 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'b0f1d0e04ec644b4bb0717d5b7b67826', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run delicate-wolf-115 at: http://localhost:5000/#/experiments/535394779431182411/runs/4e55bae30f3349ae86fcdce3e1b0a7fd\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/535394779431182411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/16 17:59:36 WARNING mlflow.sklearn: Training metrics will not be recorded because training labels were not specified. To automatically record training metrics, provide training labels as inputs to the model training function.\n",
      "2025/04/16 17:59:36 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run stylish-cow-841 at: http://localhost:5000/#/experiments/535394779431182411/runs/b0f1d0e04ec644b4bb0717d5b7b67826\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/535394779431182411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/16 17:59:42 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'b3953979673548d8a404393bf57de845', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 3.821\n",
      "MSE: 72.702\n",
      "R2: 0.732\n",
      "########## Fold: 5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/16 17:59:43 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2025/04/16 17:59:45 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '546f2c8ef0394c6693dcb1dc4ac012bb', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run spiffy-shoat-616 at: http://localhost:5000/#/experiments/535394779431182411/runs/b3953979673548d8a404393bf57de845\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/535394779431182411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/16 17:59:46 WARNING mlflow.sklearn: Training metrics will not be recorded because training labels were not specified. To automatically record training metrics, provide training labels as inputs to the model training function.\n",
      "2025/04/16 17:59:46 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run wise-ray-21 at: http://localhost:5000/#/experiments/535394779431182411/runs/546f2c8ef0394c6693dcb1dc4ac012bb\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/535394779431182411\n",
      "MAE: 3.774\n",
      "MSE: 55.732\n",
      "R2: 0.761\n",
      "##### Displaying Average of Obtained Metrics : #####\n",
      "Average MAE: 3.829 +/- 0.054\n",
      "Average MSE: 64.788 +/- 5.648\n",
      "Average R2: 0.744 +/- 0.010\n"
     ]
    }
   ],
   "source": [
    "# Modelo XGBoost\n",
    "cross_validation(X, y, model_XGBoost, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3597d6c4",
   "metadata": {},
   "source": [
    "### Modelo Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd96d119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "MAE: 5.352\n",
      "MSE: 110.291\n",
      "R2: 0.552\n",
      "########## Fold: 2 ##########\n",
      "MAE: 5.520\n",
      "MSE: 117.569\n",
      "R2: 0.555\n",
      "########## Fold: 3 ##########\n",
      "MAE: 5.452\n",
      "MSE: 108.974\n",
      "R2: 0.560\n",
      "########## Fold: 4 ##########\n",
      "MAE: 5.403\n",
      "MSE: 127.166\n",
      "R2: 0.532\n",
      "########## Fold: 5 ##########\n",
      "MAE: 5.321\n",
      "MSE: 100.226\n",
      "R2: 0.571\n",
      "##### Displaying Average of Obtained Metrics : #####\n",
      "Average MAE: 5.410 +/- 0.071\n",
      "Average MSE: 112.845 +/- 9.035\n",
      "Average R2: 0.554 +/- 0.013\n"
     ]
    }
   ],
   "source": [
    "cross_validation(X, y, model_Catboost, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db2bc48",
   "metadata": {},
   "source": [
    "### Modelo LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4c1f731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "MAE: 5.664\n",
      "MSE: 120.227\n",
      "R2: 0.511\n",
      "########## Fold: 2 ##########\n",
      "MAE: 5.827\n",
      "MSE: 128.344\n",
      "R2: 0.515\n",
      "########## Fold: 3 ##########\n",
      "MAE: 5.773\n",
      "MSE: 119.646\n",
      "R2: 0.517\n",
      "########## Fold: 4 ##########\n",
      "MAE: 5.715\n",
      "MSE: 136.550\n",
      "R2: 0.497\n",
      "########## Fold: 5 ##########\n",
      "MAE: 5.612\n",
      "MSE: 108.576\n",
      "R2: 0.535\n",
      "##### Displaying Average of Obtained Metrics : #####\n",
      "Average MAE: 5.718 +/- 0.076\n",
      "Average MSE: 122.669 +/- 9.366\n",
      "Average R2: 0.515 +/- 0.012\n"
     ]
    }
   ],
   "source": [
    "# Modelo LightGBM\n",
    "cross_validation(X, y, model_LightGBM, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5ca5e0",
   "metadata": {},
   "source": [
    "### Modelo Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ae495d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "MAE: 5.664\n",
      "MSE: 120.227\n",
      "R2: 0.511\n",
      "########## Fold: 2 ##########\n",
      "MAE: 5.827\n",
      "MSE: 128.344\n",
      "R2: 0.515\n",
      "########## Fold: 3 ##########\n",
      "MAE: 5.773\n",
      "MSE: 119.646\n",
      "R2: 0.517\n",
      "########## Fold: 4 ##########\n",
      "MAE: 5.715\n",
      "MSE: 136.550\n",
      "R2: 0.497\n",
      "########## Fold: 5 ##########\n",
      "MAE: 5.612\n",
      "MSE: 108.576\n",
      "R2: 0.535\n",
      "##### Displaying Average of Obtained Metrics : #####\n",
      "Average MAE: 5.718 +/- 0.076\n",
      "Average MSE: 122.669 +/- 9.366\n",
      "Average R2: 0.515 +/- 0.012\n"
     ]
    }
   ],
   "source": [
    "# Modelo Decision Tree\n",
    "cross_validation(X, y, model_LightGBM, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58121e7e",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a24d8dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInit signature:\u001b[39m\n",
      "XGBRegressor(\n",
      "    *,\n",
      "    objective: Union[str, xgboost.sklearn._SklObjWProto, Callable[[Any, Any], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = \u001b[33m'reg:squarederror'\u001b[39m,\n",
      "    **kwargs: Any,\n",
      ") -> \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mDocstring:\u001b[39m     \n",
      "Implementation of the scikit-learn API for XGBoost regression.\n",
      "See :doc:`/python/sklearn_estimator` for more information.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "\n",
      "    n_estimators : typing.Optional[int]\n",
      "        Number of gradient boosted trees.  Equivalent to number of boosting\n",
      "        rounds.\n",
      "\n",
      "    max_depth :  typing.Optional[int]\n",
      "\n",
      "        Maximum tree depth for base learners.\n",
      "\n",
      "    max_leaves : typing.Optional[int]\n",
      "\n",
      "        Maximum number of leaves; 0 indicates no limit.\n",
      "\n",
      "    max_bin : typing.Optional[int]\n",
      "\n",
      "        If using histogram-based algorithm, maximum number of bins per feature\n",
      "\n",
      "    grow_policy : typing.Optional[str]\n",
      "\n",
      "        Tree growing policy.\n",
      "\n",
      "        - depthwise: Favors splitting at nodes closest to the node,\n",
      "        - lossguide: Favors splitting at nodes with highest loss change.\n",
      "\n",
      "    learning_rate : typing.Optional[float]\n",
      "\n",
      "        Boosting learning rate (xgb's \"eta\")\n",
      "\n",
      "    verbosity : typing.Optional[int]\n",
      "\n",
      "        The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "\n",
      "    objective : typing.Union[str, xgboost.sklearn._SklObjWProto, typing.Callable[[typing.Any, typing.Any], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
      "\n",
      "        Specify the learning task and the corresponding learning objective or a custom\n",
      "        objective function to be used.\n",
      "\n",
      "        For custom objective, see :doc:`/tutorials/custom_metric_obj` and\n",
      "        :ref:`custom-obj-metric` for more information, along with the end note for\n",
      "        function signatures.\n",
      "\n",
      "    booster: typing.Optional[str]\n",
      "\n",
      "        Specify which booster to use: ``gbtree``, ``gblinear`` or ``dart``.\n",
      "\n",
      "    tree_method : typing.Optional[str]\n",
      "\n",
      "        Specify which tree method to use.  Default to auto.  If this parameter is set to\n",
      "        default, XGBoost will choose the most conservative option available.  It's\n",
      "        recommended to study this option from the parameters document :doc:`tree method\n",
      "        </treemethod>`\n",
      "\n",
      "    n_jobs : typing.Optional[int]\n",
      "\n",
      "        Number of parallel threads used to run xgboost.  When used with other\n",
      "        Scikit-Learn algorithms like grid search, you may choose which algorithm to\n",
      "        parallelize and balance the threads.  Creating thread contention will\n",
      "        significantly slow down both algorithms.\n",
      "\n",
      "    gamma : typing.Optional[float]\n",
      "\n",
      "        (min_split_loss) Minimum loss reduction required to make a further partition on\n",
      "        a leaf node of the tree.\n",
      "\n",
      "    min_child_weight : typing.Optional[float]\n",
      "\n",
      "        Minimum sum of instance weight(hessian) needed in a child.\n",
      "\n",
      "    max_delta_step : typing.Optional[float]\n",
      "\n",
      "        Maximum delta step we allow each tree's weight estimation to be.\n",
      "\n",
      "    subsample : typing.Optional[float]\n",
      "\n",
      "        Subsample ratio of the training instance.\n",
      "\n",
      "    sampling_method : typing.Optional[str]\n",
      "\n",
      "        Sampling method. Used only by the GPU version of ``hist`` tree method.\n",
      "\n",
      "        - ``uniform``: Select random training instances uniformly.\n",
      "        - ``gradient_based``: Select random training instances with higher probability\n",
      "            when the gradient and hessian are larger. (cf. CatBoost)\n",
      "\n",
      "    colsample_bytree : typing.Optional[float]\n",
      "\n",
      "        Subsample ratio of columns when constructing each tree.\n",
      "\n",
      "    colsample_bylevel : typing.Optional[float]\n",
      "\n",
      "        Subsample ratio of columns for each level.\n",
      "\n",
      "    colsample_bynode : typing.Optional[float]\n",
      "\n",
      "        Subsample ratio of columns for each split.\n",
      "\n",
      "    reg_alpha : typing.Optional[float]\n",
      "\n",
      "        L1 regularization term on weights (xgb's alpha).\n",
      "\n",
      "    reg_lambda : typing.Optional[float]\n",
      "\n",
      "        L2 regularization term on weights (xgb's lambda).\n",
      "\n",
      "    scale_pos_weight : typing.Optional[float]\n",
      "        Balancing of positive and negative weights.\n",
      "\n",
      "    base_score : typing.Optional[float]\n",
      "\n",
      "        The initial prediction score of all instances, global bias.\n",
      "\n",
      "    random_state : typing.Union[numpy.random.mtrand.RandomState, numpy.random._generator.Generator, int, NoneType]\n",
      "\n",
      "        Random number seed.\n",
      "\n",
      "        .. note::\n",
      "\n",
      "           Using gblinear booster with shotgun updater is nondeterministic as\n",
      "           it uses Hogwild algorithm.\n",
      "\n",
      "    missing : float\n",
      "\n",
      "        Value in the data which needs to be present as a missing value. Default to\n",
      "        :py:data:`numpy.nan`.\n",
      "\n",
      "    num_parallel_tree: typing.Optional[int]\n",
      "\n",
      "        Used for boosting random forest.\n",
      "\n",
      "    monotone_constraints : typing.Union[typing.Dict[str, int], str, NoneType]\n",
      "\n",
      "        Constraint of variable monotonicity.  See :doc:`tutorial </tutorials/monotonic>`\n",
      "        for more information.\n",
      "\n",
      "    interaction_constraints : typing.Union[str, typing.List[typing.Tuple[str]], NoneType]\n",
      "\n",
      "        Constraints for interaction representing permitted interactions.  The\n",
      "        constraints must be specified in the form of a nested list, e.g. ``[[0, 1], [2,\n",
      "        3, 4]]``, where each inner list is a group of indices of features that are\n",
      "        allowed to interact with each other.  See :doc:`tutorial\n",
      "        </tutorials/feature_interaction_constraint>` for more information\n",
      "\n",
      "    importance_type: typing.Optional[str]\n",
      "\n",
      "        The feature importance type for the feature_importances\\_ property:\n",
      "\n",
      "        * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
      "          \"total_cover\".\n",
      "        * For linear model, only \"weight\" is defined and it's the normalized\n",
      "          coefficients without bias.\n",
      "\n",
      "    device : typing.Optional[str]\n",
      "\n",
      "        .. versionadded:: 2.0.0\n",
      "\n",
      "        Device ordinal, available options are `cpu`, `cuda`, and `gpu`.\n",
      "\n",
      "    validate_parameters : typing.Optional[bool]\n",
      "\n",
      "        Give warnings for unknown parameter.\n",
      "\n",
      "    enable_categorical : bool\n",
      "\n",
      "        See the same parameter of :py:class:`DMatrix` for details.\n",
      "\n",
      "    feature_types : typing.Optional[typing.Sequence[str]]\n",
      "\n",
      "        .. versionadded:: 1.7.0\n",
      "\n",
      "        Used for specifying feature types without constructing a dataframe. See\n",
      "        :py:class:`DMatrix` for details.\n",
      "\n",
      "    feature_weights : Optional[ArrayLike]\n",
      "\n",
      "        Weight for each feature, defines the probability of each feature being selected\n",
      "        when colsample is being used.  All values must be greater than 0, otherwise a\n",
      "        `ValueError` is thrown.\n",
      "\n",
      "    max_cat_to_onehot : Optional[int]\n",
      "\n",
      "        .. versionadded:: 1.6.0\n",
      "\n",
      "        .. note:: This parameter is experimental\n",
      "\n",
      "        A threshold for deciding whether XGBoost should use one-hot encoding based split\n",
      "        for categorical data.  When number of categories is lesser than the threshold\n",
      "        then one-hot encoding is chosen, otherwise the categories will be partitioned\n",
      "        into children nodes. Also, `enable_categorical` needs to be set to have\n",
      "        categorical feature support. See :doc:`Categorical Data\n",
      "        </tutorials/categorical>` and :ref:`cat-param` for details.\n",
      "\n",
      "    max_cat_threshold : typing.Optional[int]\n",
      "\n",
      "        .. versionadded:: 1.7.0\n",
      "\n",
      "        .. note:: This parameter is experimental\n",
      "\n",
      "        Maximum number of categories considered for each split. Used only by\n",
      "        partition-based splits for preventing over-fitting. Also, `enable_categorical`\n",
      "        needs to be set to have categorical feature support. See :doc:`Categorical Data\n",
      "        </tutorials/categorical>` and :ref:`cat-param` for details.\n",
      "\n",
      "    multi_strategy : typing.Optional[str]\n",
      "\n",
      "        .. versionadded:: 2.0.0\n",
      "\n",
      "        .. note:: This parameter is working-in-progress.\n",
      "\n",
      "        The strategy used for training multi-target models, including multi-target\n",
      "        regression and multi-class classification. See :doc:`/tutorials/multioutput` for\n",
      "        more information.\n",
      "\n",
      "        - ``one_output_per_tree``: One model for each target.\n",
      "        - ``multi_output_tree``:  Use multi-target trees.\n",
      "\n",
      "    eval_metric : typing.Union[str, typing.List[str], typing.Callable, NoneType]\n",
      "\n",
      "        .. versionadded:: 1.6.0\n",
      "\n",
      "        Metric used for monitoring the training result and early stopping.  It can be a\n",
      "        string or list of strings as names of predefined metric in XGBoost (See\n",
      "        :doc:`/parameter`), one of the metrics in :py:mod:`sklearn.metrics`, or any\n",
      "        other user defined metric that looks like `sklearn.metrics`.\n",
      "\n",
      "        If custom objective is also provided, then custom metric should implement the\n",
      "        corresponding reverse link function.\n",
      "\n",
      "        Unlike the `scoring` parameter commonly used in scikit-learn, when a callable\n",
      "        object is provided, it's assumed to be a cost function and by default XGBoost\n",
      "        will minimize the result during early stopping.\n",
      "\n",
      "        For advanced usage on Early stopping like directly choosing to maximize instead\n",
      "        of minimize, see :py:obj:`xgboost.callback.EarlyStopping`.\n",
      "\n",
      "        See :doc:`/tutorials/custom_metric_obj` and :ref:`custom-obj-metric` for more\n",
      "        information.\n",
      "\n",
      "        .. code-block:: python\n",
      "\n",
      "            from sklearn.datasets import load_diabetes\n",
      "            from sklearn.metrics import mean_absolute_error\n",
      "            X, y = load_diabetes(return_X_y=True)\n",
      "            reg = xgb.XGBRegressor(\n",
      "                tree_method=\"hist\",\n",
      "                eval_metric=mean_absolute_error,\n",
      "            )\n",
      "            reg.fit(X, y, eval_set=[(X, y)])\n",
      "\n",
      "    early_stopping_rounds : typing.Optional[int]\n",
      "\n",
      "        .. versionadded:: 1.6.0\n",
      "\n",
      "        - Activates early stopping. Validation metric needs to improve at least once in\n",
      "          every **early_stopping_rounds** round(s) to continue training.  Requires at\n",
      "          least one item in **eval_set** in :py:meth:`fit`.\n",
      "\n",
      "        - If early stopping occurs, the model will have two additional attributes:\n",
      "          :py:attr:`best_score` and :py:attr:`best_iteration`. These are used by the\n",
      "          :py:meth:`predict` and :py:meth:`apply` methods to determine the optimal\n",
      "          number of trees during inference. If users want to access the full model\n",
      "          (including trees built after early stopping), they can specify the\n",
      "          `iteration_range` in these inference methods. In addition, other utilities\n",
      "          like model plotting can also use the entire model.\n",
      "\n",
      "        - If you prefer to discard the trees after `best_iteration`, consider using the\n",
      "          callback function :py:class:`xgboost.callback.EarlyStopping`.\n",
      "\n",
      "        - If there's more than one item in **eval_set**, the last entry will be used for\n",
      "          early stopping.  If there's more than one metric in **eval_metric**, the last\n",
      "          metric will be used for early stopping.\n",
      "\n",
      "    callbacks : typing.Optional[typing.List[xgboost.callback.TrainingCallback]]\n",
      "\n",
      "        List of callback functions that are applied at end of each iteration.\n",
      "        It is possible to use predefined callbacks by using\n",
      "        :ref:`Callback API <callback_api>`.\n",
      "\n",
      "        .. note::\n",
      "\n",
      "           States in callback are not preserved during training, which means callback\n",
      "           objects can not be reused for multiple training sessions without\n",
      "           reinitialization or deepcopy.\n",
      "\n",
      "        .. code-block:: python\n",
      "\n",
      "            for params in parameters_grid:\n",
      "                # be sure to (re)initialize the callbacks before each run\n",
      "                callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]\n",
      "                reg = xgboost.XGBRegressor(**params, callbacks=callbacks)\n",
      "                reg.fit(X, y)\n",
      "\n",
      "    kwargs : typing.Optional[typing.Any]\n",
      "\n",
      "        Keyword arguments for XGBoost Booster object.  Full documentation of parameters\n",
      "        can be found :doc:`here </parameter>`.\n",
      "        Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
      "        dict simultaneously will result in a TypeError.\n",
      "\n",
      "        .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "\n",
      "            \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
      "            that parameters passed via this argument will interact properly\n",
      "            with scikit-learn.\n",
      "\n",
      "        .. note::  Custom objective function\n",
      "\n",
      "            A custom objective function can be provided for the ``objective``\n",
      "            parameter. In this case, it should have the signature ``objective(y_true,\n",
      "            y_pred) -> [grad, hess]`` or ``objective(y_true, y_pred, *, sample_weight)\n",
      "            -> [grad, hess]``:\n",
      "\n",
      "            y_true: array_like of shape [n_samples]\n",
      "                The target values\n",
      "            y_pred: array_like of shape [n_samples]\n",
      "                The predicted values\n",
      "            sample_weight :\n",
      "                Optional sample weights.\n",
      "\n",
      "            grad: array_like of shape [n_samples]\n",
      "                The value of the gradient for each sample point.\n",
      "            hess: array_like of shape [n_samples]\n",
      "                The value of the second derivative for each sample point\n",
      "\n",
      "            Note that, if the custom objective produces negative values for\n",
      "            the Hessian, these will be clipped. If the objective is non-convex,\n",
      "            one might also consider using the expected Hessian (Fisher\n",
      "            information) instead.\n",
      "\u001b[31mFile:\u001b[39m           /opt/conda/envs/olist-freight/lib/python3.13/site-packages/xgboost/sklearn.py\n",
      "\u001b[31mType:\u001b[39m           type\n",
      "\u001b[31mSubclasses:\u001b[39m     XGBRFRegressor"
     ]
    }
   ],
   "source": [
    "?XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b05d0a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:55:45,010] A new study created in memory with name: no-name-5d095bf9-e599-4aa5-bec4-1e8d3ff7bed7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "########## Fold: 2 ##########\n",
      "########## Fold: 3 ##########\n",
      "########## Fold: 4 ##########\n",
      "########## Fold: 5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:56:30,932] Trial 0 finished with value: 100.96271617963595 and parameters: {'learning_rate': 0.008335068262687652, 'max_depth': 5, 'subsample': 0.6, 'colsample_bytree': 0.9, 'min_child_weight': 4}. Best is trial 0 with value: 100.96271617963595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "########## Fold: 2 ##########\n",
      "########## Fold: 3 ##########\n",
      "########## Fold: 4 ##########\n",
      "########## Fold: 5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:57:16,785] Trial 1 finished with value: 100.96271617963595 and parameters: {'learning_rate': 0.006119232988146066, 'max_depth': 10, 'subsample': 0.5, 'colsample_bytree': 0.6, 'min_child_weight': 3}. Best is trial 0 with value: 100.96271617963595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "########## Fold: 2 ##########\n",
      "########## Fold: 3 ##########\n",
      "########## Fold: 4 ##########\n",
      "########## Fold: 5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:58:02,090] Trial 2 finished with value: 100.96271617963595 and parameters: {'learning_rate': 0.0034636305272420635, 'max_depth': 9, 'subsample': 1.0, 'colsample_bytree': 0.5, 'min_child_weight': 4}. Best is trial 0 with value: 100.96271617963595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "########## Fold: 2 ##########\n",
      "########## Fold: 3 ##########\n",
      "########## Fold: 4 ##########\n",
      "########## Fold: 5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:58:47,717] Trial 3 finished with value: 100.96271617963595 and parameters: {'learning_rate': 0.005697358260035766, 'max_depth': 7, 'subsample': 0.6, 'colsample_bytree': 0.8, 'min_child_weight': 10}. Best is trial 0 with value: 100.96271617963595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "########## Fold: 2 ##########\n",
      "########## Fold: 3 ##########\n",
      "########## Fold: 4 ##########\n",
      "########## Fold: 5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:59:33,414] Trial 4 finished with value: 100.96271617963595 and parameters: {'learning_rate': 0.003344880924745632, 'max_depth': 10, 'subsample': 0.6, 'colsample_bytree': 0.6, 'min_child_weight': 10}. Best is trial 0 with value: 100.96271617963595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "########## Fold: 2 ##########\n",
      "########## Fold: 3 ##########\n",
      "########## Fold: 4 ##########\n",
      "########## Fold: 5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 17:00:18,873] Trial 5 finished with value: 100.96271617963595 and parameters: {'learning_rate': 0.01200696680026771, 'max_depth': 1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'min_child_weight': 8}. Best is trial 0 with value: 100.96271617963595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "########## Fold: 2 ##########\n",
      "########## Fold: 3 ##########\n",
      "########## Fold: 4 ##########\n",
      "########## Fold: 5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 17:01:04,625] Trial 6 finished with value: 100.96271617963595 and parameters: {'learning_rate': 0.0030691185221282773, 'max_depth': 5, 'subsample': 0.8, 'colsample_bytree': 0.5, 'min_child_weight': 9}. Best is trial 0 with value: 100.96271617963595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "########## Fold: 2 ##########\n",
      "########## Fold: 3 ##########\n",
      "########## Fold: 4 ##########\n",
      "########## Fold: 5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 17:01:50,346] Trial 7 finished with value: 100.96271617963595 and parameters: {'learning_rate': 0.09468144339979574, 'max_depth': 7, 'subsample': 0.7, 'colsample_bytree': 0.8, 'min_child_weight': 7}. Best is trial 0 with value: 100.96271617963595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "########## Fold: 2 ##########\n",
      "########## Fold: 3 ##########\n",
      "########## Fold: 4 ##########\n",
      "########## Fold: 5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 17:02:48,160] Trial 8 finished with value: 100.96271617963595 and parameters: {'learning_rate': 0.06958181703505487, 'max_depth': 9, 'subsample': 0.9, 'colsample_bytree': 0.5, 'min_child_weight': 10}. Best is trial 0 with value: 100.96271617963595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "########## Fold: 2 ##########\n",
      "########## Fold: 3 ##########\n",
      "########## Fold: 4 ##########\n",
      "########## Fold: 5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 17:03:41,373] Trial 9 finished with value: 100.96271617963595 and parameters: {'learning_rate': 0.007804584388729891, 'max_depth': 6, 'subsample': 0.6, 'colsample_bytree': 0.7, 'min_child_weight': 6}. Best is trial 0 with value: 100.96271617963595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "########## Fold: 2 ##########\n",
      "########## Fold: 3 ##########\n",
      "########## Fold: 4 ##########\n",
      "########## Fold: 5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 17:04:27,987] Trial 10 finished with value: 100.96271617963595 and parameters: {'learning_rate': 0.001273247204695463, 'max_depth': 3, 'subsample': 0.5, 'colsample_bytree': 1.0, 'min_child_weight': 2}. Best is trial 0 with value: 100.96271617963595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "########## Fold: 2 ##########\n",
      "########## Fold: 3 ##########\n",
      "########## Fold: 4 ##########\n",
      "########## Fold: 5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 17:05:13,992] Trial 11 finished with value: 100.96271617963595 and parameters: {'learning_rate': 0.030150621300125302, 'max_depth': 4, 'subsample': 0.5, 'colsample_bytree': 1.0, 'min_child_weight': 3}. Best is trial 0 with value: 100.96271617963595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "########## Fold: 2 ##########\n",
      "########## Fold: 3 ##########\n",
      "########## Fold: 4 ##########\n",
      "########## Fold: 5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 17:06:00,354] Trial 12 finished with value: 100.96271617963595 and parameters: {'learning_rate': 0.01654439614305352, 'max_depth': 3, 'subsample': 0.5, 'colsample_bytree': 0.9, 'min_child_weight': 1}. Best is trial 0 with value: 100.96271617963595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "########## Fold: 2 ##########\n",
      "########## Fold: 3 ##########\n",
      "########## Fold: 4 ##########\n",
      "########## Fold: 5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 17:06:48,371] Trial 13 finished with value: 100.96271617963595 and parameters: {'learning_rate': 0.025899880362387308, 'max_depth': 8, 'subsample': 0.6, 'colsample_bytree': 0.7, 'min_child_weight': 4}. Best is trial 0 with value: 100.96271617963595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "########## Fold: 2 ##########\n",
      "########## Fold: 3 ##########\n",
      "########## Fold: 4 ##########\n",
      "########## Fold: 5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 17:07:47,710] Trial 14 finished with value: 100.96271617963595 and parameters: {'learning_rate': 0.0010751044269778122, 'max_depth': 5, 'subsample': 0.8, 'colsample_bytree': 0.9, 'min_child_weight': 5}. Best is trial 0 with value: 100.96271617963595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "########## Fold: 2 ##########\n",
      "########## Fold: 3 ##########\n",
      "########## Fold: 4 ##########\n",
      "########## Fold: 5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 17:08:43,427] Trial 15 finished with value: 100.96271617963595 and parameters: {'learning_rate': 0.0053961193844775665, 'max_depth': 1, 'subsample': 0.5, 'colsample_bytree': 0.6, 'min_child_weight': 3}. Best is trial 0 with value: 100.96271617963595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "########## Fold: 2 ##########\n",
      "########## Fold: 3 ##########\n",
      "########## Fold: 4 ##########\n",
      "########## Fold: 5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 17:09:33,376] Trial 16 finished with value: 100.96271617963595 and parameters: {'learning_rate': 0.01686601742394893, 'max_depth': 10, 'subsample': 0.7, 'colsample_bytree': 0.9, 'min_child_weight': 1}. Best is trial 0 with value: 100.96271617963595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "########## Fold: 2 ##########\n",
      "########## Fold: 3 ##########\n",
      "########## Fold: 4 ##########\n",
      "########## Fold: 5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 17:10:30,768] Trial 17 finished with value: 100.96271617963595 and parameters: {'learning_rate': 0.0018858714912222496, 'max_depth': 6, 'subsample': 0.6, 'colsample_bytree': 0.6, 'min_child_weight': 5}. Best is trial 0 with value: 100.96271617963595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "########## Fold: 2 ##########\n",
      "########## Fold: 3 ##########\n",
      "########## Fold: 4 ##########\n",
      "########## Fold: 5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 17:11:20,080] Trial 18 finished with value: 100.96271617963595 and parameters: {'learning_rate': 0.008382868491898527, 'max_depth': 4, 'subsample': 0.5, 'colsample_bytree': 0.7, 'min_child_weight': 3}. Best is trial 0 with value: 100.96271617963595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "########## Fold: 2 ##########\n",
      "########## Fold: 3 ##########\n",
      "########## Fold: 4 ##########\n",
      "########## Fold: 5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 17:12:06,901] Trial 19 finished with value: 100.96271617963595 and parameters: {'learning_rate': 0.0302625415564257, 'max_depth': 8, 'subsample': 0.7, 'colsample_bytree': 0.9, 'min_child_weight': 6}. Best is trial 0 with value: 100.96271617963595.\n"
     ]
    }
   ],
   "source": [
    "def fine_tuning(trial, k=5):\n",
    "    # tuning\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True)\n",
    "    max_depth = trial.suggest_int('max_depth', 1, 10)\n",
    "    subsample = trial.suggest_float('subsample', 0.5, 1, step=0.1)\n",
    "    colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1, step=0.1)\n",
    "    min_child_weight = trial.suggest_int('min_child_weight', 1, 10)\n",
    "\n",
    "    folds = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    absolute_errors = list()\n",
    "    squared_errors = list()\n",
    "    r2 = list()\n",
    "\n",
    "    for k, (train_index, test_index) in enumerate(folds.split(X, y)):\n",
    "        \n",
    "        print(\"#\"*10 + f\" Fold: {k+1} \" + \"#\"*10)\n",
    "        \n",
    "        X_train_internal, y_train_internal = X.iloc[train_index, :], y.iloc[train_index]\n",
    "        X_test_internal, y_test_internal = X.iloc[test_index, :], y.iloc[test_index]\n",
    "\n",
    "        encoder = CatBoostEncoder()\n",
    "        \n",
    "        cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "        num_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "        cat_pipeline = Pipeline([('encoder', encoder), ('imputer', cat_imputer)])\n",
    "        num_pipeline = Pipeline([('imputer', num_imputer)])\n",
    "\n",
    "        cat_cols = X_train_internal.select_dtypes(include=['object']).columns\n",
    "        num_cols = X_train_internal.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "        X_train_internal[cat_cols] = cat_pipeline.fit_transform(X_train_internal[cat_cols], y_train_internal)\n",
    "        X_train_internal[num_cols] = num_pipeline.fit_transform(X_train_internal[num_cols])\n",
    "\n",
    "        X_test_internal[cat_cols] = cat_pipeline.transform(X_test_internal[cat_cols])\n",
    "        X_test_internal[num_cols] = num_pipeline.transform(X_test_internal[num_cols])\n",
    "\n",
    "        model_XGBoost.fit(X_train_internal, y_train_internal)\n",
    "        y_pred = model_XGBoost.predict(X_test_internal)\n",
    "        r2score = r2_score(y_test_internal, y_pred)\n",
    "        mse = mean_squared_error(y_test_internal, y_pred)\n",
    "        mae = mean_absolute_error(y_test_internal, y_pred)\n",
    "\n",
    "        absolute_errors.append(mae)\n",
    "        squared_errors.append(mse)\n",
    "        r2.append(r2score)\n",
    "\n",
    "    \n",
    "    absolute_errors = np.array(absolute_errors)\n",
    "    squared_errors = np.array(squared_errors)\n",
    "    r2 = np.array(r2)\n",
    "\n",
    "    avg_mae = np.mean(absolute_errors)\n",
    "    avg_mse = np.mean(squared_errors)\n",
    "    avg_r2 = np.mean(r2)\n",
    "\n",
    "    std_mae = np.std(absolute_errors)\n",
    "    std_mse = np.std(squared_errors)\n",
    "    std_r2 = np.std(r2)\n",
    "\n",
    "    return avg_mse\n",
    "\n",
    "study = opt.create_study(direction='minimize')\n",
    "study.optimize(fine_tuning, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "641a9285",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'learning_rate': 0.008335068262687652, 'max_depth': 5, 'subsample': 0.6, 'colsample_bytree': 0.9, 'min_child_weight': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26068523",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_XGBoost = XGBRegressor(n_estimators=1000, n_jobs=-1, random_state=0, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0d3672a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "MAE: 3.785\n",
      "MSE: 64.887\n",
      "R2: 0.736\n",
      "########## Fold: 2 ##########\n",
      "MAE: 3.925\n",
      "MSE: 67.987\n",
      "R2: 0.743\n",
      "########## Fold: 3 ##########\n",
      "MAE: 3.843\n",
      "MSE: 62.634\n",
      "R2: 0.747\n",
      "########## Fold: 4 ##########\n",
      "MAE: 3.821\n",
      "MSE: 72.702\n",
      "R2: 0.732\n",
      "########## Fold: 5 ##########\n",
      "MAE: 3.774\n",
      "MSE: 55.732\n",
      "R2: 0.761\n",
      "##### Displaying Average of Obtained Metrics : #####\n",
      "Average MAE: 3.829 +/- 0.054\n",
      "Average MSE: 64.788 +/- 5.648\n",
      "Average R2: 0.744 +/- 0.010\n"
     ]
    }
   ],
   "source": [
    "cross_validation(X, y, model_XGBoost, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb63e23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(X_train, X_test):\n",
    "\n",
    "    X_train['volume'] = X_train['product_length_cm'] * X_train['product_height_cm'] * X_train['product_width_cm']\n",
    "    X_test['volume'] = X_test['product_length_cm'] * X_test['product_height_cm'] * X_test['product_width_cm']\n",
    "\n",
    "    # other important features\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7024062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(X, y, model, k):\n",
    "    folds = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    absolute_errors = list()\n",
    "    squared_errors = list()\n",
    "    r2 = list()\n",
    "\n",
    "    for k, (train_index, test_index) in enumerate(folds.split(X, y)):\n",
    "        \n",
    "        print(\"#\"*10 + f\" Fold: {k+1} \" + \"#\"*10)\n",
    "        \n",
    "        X_train_internal, y_train_internal = X.iloc[train_index, :], y.iloc[train_index]\n",
    "        X_test_internal, y_test_internal = X.iloc[test_index, :], y.iloc[test_index]\n",
    "\n",
    "        encoder = CatBoostEncoder()\n",
    "        \n",
    "        cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "        num_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "        cat_pipeline = Pipeline([('encoder', encoder), ('imputer', cat_imputer)])\n",
    "        num_pipeline = Pipeline([('imputer', num_imputer)])\n",
    "\n",
    "        X_train_internal, X_test_internal = feature_engineering(X_train_internal, X_test_internal)\n",
    "\n",
    "        cat_cols = X_train_internal.select_dtypes(include=['object']).columns\n",
    "        num_cols = X_train_internal.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "        X_train_internal[cat_cols] = cat_pipeline.fit_transform(X_train_internal[cat_cols], y_train_internal)\n",
    "        X_train_internal[num_cols] = num_pipeline.fit_transform(X_train_internal[num_cols])\n",
    "\n",
    "        X_test_internal[cat_cols] = cat_pipeline.transform(X_test_internal[cat_cols])\n",
    "        X_test_internal[num_cols] = num_pipeline.transform(X_test_internal[num_cols])\n",
    "\n",
    "        model.fit(X_train_internal, y_train_internal)\n",
    "        y_pred = model.predict(X_test_internal)\n",
    "        r2score = r2_score(y_test_internal, y_pred)\n",
    "        mse = mean_squared_error(y_test_internal, y_pred)\n",
    "        mae = mean_absolute_error(y_test_internal, y_pred)\n",
    "\n",
    "        absolute_errors.append(mae)\n",
    "        squared_errors.append(mse)\n",
    "        r2.append(r2score)\n",
    "\n",
    "        print(f'MAE: {mae:.3f}')\n",
    "        print(f'MSE: {mse:.3f}')\n",
    "        print(f'R2: {r2score:.3f}')\n",
    "    \n",
    "    absolute_errors = np.array(absolute_errors)\n",
    "    squared_errors = np.array(squared_errors)\n",
    "    r2 = np.array(r2)\n",
    "\n",
    "    avg_mae = np.mean(absolute_errors)\n",
    "    avg_mse = np.mean(squared_errors)\n",
    "    avg_r2 = np.mean(r2)\n",
    "\n",
    "    std_mae = np.std(absolute_errors)\n",
    "    std_mse = np.std(squared_errors)\n",
    "    std_r2 = np.std(r2)\n",
    "\n",
    "    print(\"#\"*5 + f\" Displaying Average of Obtained Metrics : \" + \"#\"*5)\n",
    "    print(f\"Average MAE: {avg_mae:.3f} +/- {std_mae:.3f}\")\n",
    "    print(f'Average MSE: {avg_mse:.3f} +/- {std_mse:.3f}')\n",
    "    print(f'Average R2: {avg_r2:.3f} +/- {std_r2:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74725c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "MAE: 5.664\n",
      "MSE: 120.227\n",
      "R2: 0.511\n",
      "########## Fold: 2 ##########\n",
      "MAE: 5.827\n",
      "MSE: 128.344\n",
      "R2: 0.515\n",
      "########## Fold: 3 ##########\n",
      "MAE: 5.773\n",
      "MSE: 119.646\n",
      "R2: 0.517\n",
      "########## Fold: 4 ##########\n",
      "MAE: 5.715\n",
      "MSE: 136.550\n",
      "R2: 0.497\n",
      "########## Fold: 5 ##########\n",
      "MAE: 5.612\n",
      "MSE: 108.576\n",
      "R2: 0.535\n",
      "##### Displaying Average of Obtained Metrics : #####\n",
      "Average MAE: 5.718 +/- 0.076\n",
      "Average MSE: 122.669 +/- 9.366\n",
      "Average R2: 0.515 +/- 0.012\n"
     ]
    }
   ],
   "source": [
    "cross_validation(X, y, model_LightGBM, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2b7c11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olist-freight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
