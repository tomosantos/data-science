{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3baa96b9",
   "metadata": {},
   "source": [
    "# Freight Value Prediction Model\n",
    "This notebook focuses on building and evaluating machine learning models to predict freight values using the Olist dataset. The steps include data preparation, feature engineering, model training, evaluation, and fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a206f9",
   "metadata": {},
   "source": [
    "## 1. Importing Required Libraries\n",
    "We start by importing the necessary libraries for data manipulation, visualization, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed7adfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/olist-freight/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from category_encoders import CatBoostEncoder\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import optuna as opt\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a544e5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/535394779431182411', creation_time=1744772924702, experiment_id='535394779431182411', last_update_time=1744772924702, lifecycle_stage='active', name='freight-prediction', tags={}>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "import mlflow.lightgbm\n",
    "import mlflow.catboost\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000/\")\n",
    "mlflow.set_experiment(experiment_id=535394779431182411)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ea7eef",
   "metadata": {},
   "source": [
    "## 2. Loading the Dataset\n",
    "We load the preprocessed dataset `df_final.csv` from the `data` directory. This dataset contains features and the target variable `freight_value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdef6767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>freight_value</th>\n",
       "      <th>product_category_name</th>\n",
       "      <th>product_weight_g</th>\n",
       "      <th>product_length_cm</th>\n",
       "      <th>product_height_cm</th>\n",
       "      <th>product_width_cm</th>\n",
       "      <th>customer_city</th>\n",
       "      <th>customer_state</th>\n",
       "      <th>review_score</th>\n",
       "      <th>...</th>\n",
       "      <th>volume</th>\n",
       "      <th>density</th>\n",
       "      <th>actual_delivery_time</th>\n",
       "      <th>estimated_delivery_time</th>\n",
       "      <th>approval_order_time</th>\n",
       "      <th>distance</th>\n",
       "      <th>purchase_month</th>\n",
       "      <th>purchase_day_of_week</th>\n",
       "      <th>black_friday</th>\n",
       "      <th>christmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.99</td>\n",
       "      <td>8.72</td>\n",
       "      <td>utilidades_domesticas</td>\n",
       "      <td>500.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>sao paulo</td>\n",
       "      <td>SP</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>0.253036</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.566632</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118.70</td>\n",
       "      <td>22.76</td>\n",
       "      <td>perfumaria</td>\n",
       "      <td>400.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>barreiras</td>\n",
       "      <td>BA</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>4693.0</td>\n",
       "      <td>0.085233</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>847.437333</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>159.90</td>\n",
       "      <td>19.22</td>\n",
       "      <td>automotivo</td>\n",
       "      <td>420.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>vianopolis</td>\n",
       "      <td>GO</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>9576.0</td>\n",
       "      <td>0.043860</td>\n",
       "      <td>9.0</td>\n",
       "      <td>26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>512.100044</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.00</td>\n",
       "      <td>27.20</td>\n",
       "      <td>pet_shop</td>\n",
       "      <td>450.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>sao goncalo do amarante</td>\n",
       "      <td>RN</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1816.085655</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.90</td>\n",
       "      <td>8.72</td>\n",
       "      <td>papelaria</td>\n",
       "      <td>250.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>santo andre</td>\n",
       "      <td>SP</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>11475.0</td>\n",
       "      <td>0.021786</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.684401</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    price  freight_value  product_category_name  product_weight_g  \\\n",
       "0   29.99           8.72  utilidades_domesticas             500.0   \n",
       "1  118.70          22.76             perfumaria             400.0   \n",
       "2  159.90          19.22             automotivo             420.0   \n",
       "3   45.00          27.20               pet_shop             450.0   \n",
       "4   19.90           8.72              papelaria             250.0   \n",
       "\n",
       "   product_length_cm  product_height_cm  product_width_cm  \\\n",
       "0               19.0                8.0              13.0   \n",
       "1               19.0               13.0              19.0   \n",
       "2               24.0               19.0              21.0   \n",
       "3               30.0               10.0              20.0   \n",
       "4               51.0               15.0              15.0   \n",
       "\n",
       "             customer_city customer_state  review_score  ...   volume  \\\n",
       "0                sao paulo             SP             4  ...   1976.0   \n",
       "1                barreiras             BA             4  ...   4693.0   \n",
       "2               vianopolis             GO             5  ...   9576.0   \n",
       "3  sao goncalo do amarante             RN             5  ...   6000.0   \n",
       "4              santo andre             SP             5  ...  11475.0   \n",
       "\n",
       "    density  actual_delivery_time  estimated_delivery_time  \\\n",
       "0  0.253036                   8.0                       15   \n",
       "1  0.085233                  13.0                       19   \n",
       "2  0.043860                   9.0                       26   \n",
       "3  0.075000                  13.0                       26   \n",
       "4  0.021786                   2.0                       12   \n",
       "\n",
       "   approval_order_time     distance  purchase_month  purchase_day_of_week  \\\n",
       "0                  0.0    18.566632              10                     0   \n",
       "1                  1.0   847.437333               7                     1   \n",
       "2                  0.0   512.100044               8                     2   \n",
       "3                  0.0  1816.085655              11                     5   \n",
       "4                  0.0    29.684401               2                     1   \n",
       "\n",
       "   black_friday  christmas  \n",
       "0             0          0  \n",
       "1             0          0  \n",
       "2             0          0  \n",
       "3             0          0  \n",
       "4             0          0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the base directory for the data files\n",
    "data_dir = 'data'\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(os.path.join(data_dir, 'df_final.csv'))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9cff3b",
   "metadata": {},
   "source": [
    "## 3. Splitting Data into Features and Target\n",
    "We separate the dataset into features (`X`) and the target variable (`y`), which is `freight_value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aecfa2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting in X and Y\n",
    "X = df.drop(columns='freight_value', axis=1)\n",
    "y = df.freight_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da626fab",
   "metadata": {},
   "source": [
    "## 4. Train-Test Split\n",
    "We split the data into training and testing sets using an 80-20 split to evaluate the model's performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b7ee63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting in train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f469e2",
   "metadata": {},
   "source": [
    "## 5. Initializing Models\n",
    "We initialize multiple regression models, including XGBoost, LightGBM, CatBoost, and Decision Tree, with default or predefined hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "616a435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing models\n",
    "model_XGBoost = XGBRegressor(n_estimators = 1000, max_depth = 8, learning_rate = 1e-3, random_state = 0)\n",
    "model_LightGBM = LGBMRegressor(n_estimators = 1000, max_depth = 8, num_leaves = 2^8, learning_rate = 1e-3, n_jobs = -1, verbose = -1, random_state = 0)\n",
    "model_Catboost = CatBoostRegressor(n_estimators = 1000, max_depth = 8, learning_rate = 1e-3, random_state = 0, verbose = 0)\n",
    "model_DecisionTree = DecisionTreeRegressor(random_state = 0, max_depth = 8, min_samples_split = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36a1930",
   "metadata": {},
   "source": [
    "## 6. Feature Encoding and Importance\n",
    "We encode categorical features using CatBoostEncoder and calculate feature importance using permutation importance with the XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1693328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "\n",
    "encoder = CatBoostEncoder()\n",
    "X_train_encoded = X_train.copy()\n",
    "X_test_encoded = X_test.copy()\n",
    "\n",
    "for col in X_train_encoded.select_dtypes(include=['object']).columns:\n",
    "    X_train_encoded[col] = encoder.fit_transform(X_train_encoded[col], y_train)\n",
    "    X_test_encoded[col] = encoder.transform(X_test_encoded[col])\n",
    "\n",
    "model_XGBoost.fit(X_train_encoded, y_train)\n",
    "r = permutation_importance(model_XGBoost, X_test_encoded, y_test, n_repeats=30, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66add077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>distance</td>\n",
       "      <td>0.211973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>volume</td>\n",
       "      <td>0.198656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>product_weight_g</td>\n",
       "      <td>0.178174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>price</td>\n",
       "      <td>0.080163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>customer_state</td>\n",
       "      <td>0.025644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>product_length_cm</td>\n",
       "      <td>0.022553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>seller_city</td>\n",
       "      <td>0.021714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>seller_state</td>\n",
       "      <td>0.013614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>product_width_cm</td>\n",
       "      <td>0.004467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>density</td>\n",
       "      <td>0.003321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>purchase_month</td>\n",
       "      <td>0.002923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product_category_name</td>\n",
       "      <td>0.001475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>customer_city</td>\n",
       "      <td>0.001295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>product_height_cm</td>\n",
       "      <td>0.001211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>actual_delivery_time</td>\n",
       "      <td>0.000642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>review_score</td>\n",
       "      <td>0.000312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>black_friday</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>christmas</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>purchase_day_of_week</td>\n",
       "      <td>-0.000202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>approval_order_time</td>\n",
       "      <td>-0.000902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>estimated_delivery_time</td>\n",
       "      <td>-0.001062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Feature  importance\n",
       "16                 distance    0.211973\n",
       "11                   volume    0.198656\n",
       "2          product_weight_g    0.178174\n",
       "0                     price    0.080163\n",
       "7            customer_state    0.025644\n",
       "3         product_length_cm    0.022553\n",
       "9               seller_city    0.021714\n",
       "10             seller_state    0.013614\n",
       "5          product_width_cm    0.004467\n",
       "12                  density    0.003321\n",
       "17           purchase_month    0.002923\n",
       "1     product_category_name    0.001475\n",
       "6             customer_city    0.001295\n",
       "4         product_height_cm    0.001211\n",
       "13     actual_delivery_time    0.000642\n",
       "8              review_score    0.000312\n",
       "19             black_friday    0.000003\n",
       "20                christmas    0.000000\n",
       "18     purchase_day_of_week   -0.000202\n",
       "15      approval_order_time   -0.000902\n",
       "14  estimated_delivery_time   -0.001062"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances = pd.DataFrame({'Feature': X_test_encoded.columns, 'importance': r.importances_mean})\n",
    "importances = importances.sort_values(by='importance', ascending=False)\n",
    "importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488e2047",
   "metadata": {},
   "source": [
    "## 7. Feature Selection\n",
    "Based on feature importance, we drop less important features to simplify the model and improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bceda8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "less_important_columns = ['purchase_day_of_week', 'approval_order_time', 'estimated_delivery_time', 'christmas', 'black_friday']\n",
    "\n",
    "X_train = X_train.drop(columns=less_important_columns)\n",
    "X_test = X_test.drop(columns=less_important_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d135ac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "\n",
    "encoder = CatBoostEncoder()\n",
    "X_train_encoded = X_train.copy()\n",
    "X_test_encoded = X_test.copy()\n",
    "\n",
    "for col in X_train_encoded.select_dtypes(include=['object']).columns:\n",
    "    X_train_encoded[col] = encoder.fit_transform(X_train_encoded[col], y_train)\n",
    "    X_test_encoded[col] = encoder.transform(X_test_encoded[col])\n",
    "\n",
    "model_XGBoost.fit(X_train_encoded, y_train)\n",
    "r_v2 = permutation_importance(model_XGBoost, X_test_encoded, y_test, n_repeats=30, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a41b22ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>distance</td>\n",
       "      <td>0.211323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>volume</td>\n",
       "      <td>0.203850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>product_weight_g</td>\n",
       "      <td>0.182162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>price</td>\n",
       "      <td>0.090672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>customer_state</td>\n",
       "      <td>0.024785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>product_length_cm</td>\n",
       "      <td>0.024383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>seller_city</td>\n",
       "      <td>0.022433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>seller_state</td>\n",
       "      <td>0.012966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>product_width_cm</td>\n",
       "      <td>0.004509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>customer_city</td>\n",
       "      <td>0.003811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>purchase_month</td>\n",
       "      <td>0.002410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>density</td>\n",
       "      <td>0.001630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product_category_name</td>\n",
       "      <td>0.001497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>product_height_cm</td>\n",
       "      <td>0.001340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>actual_delivery_time</td>\n",
       "      <td>0.000855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>review_score</td>\n",
       "      <td>0.000339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Feature  importance\n",
       "14               distance    0.211323\n",
       "11                 volume    0.203850\n",
       "2        product_weight_g    0.182162\n",
       "0                   price    0.090672\n",
       "7          customer_state    0.024785\n",
       "3       product_length_cm    0.024383\n",
       "9             seller_city    0.022433\n",
       "10           seller_state    0.012966\n",
       "5        product_width_cm    0.004509\n",
       "6           customer_city    0.003811\n",
       "15         purchase_month    0.002410\n",
       "12                density    0.001630\n",
       "1   product_category_name    0.001497\n",
       "4       product_height_cm    0.001340\n",
       "13   actual_delivery_time    0.000855\n",
       "8            review_score    0.000339"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances_v2 = pd.DataFrame({'Feature': X_test_encoded.columns, 'importance': r_v2.importances_mean})\n",
    "importances_v2 = importances_v2.sort_values(by='importance', ascending=False)\n",
    "importances_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2493941a",
   "metadata": {},
   "source": [
    "## 8. Cross-Validation\n",
    "We perform k-fold cross-validation to evaluate the model's performance across multiple splits of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa76170a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 5.044\n",
      "MSE: 98.518\n",
      "R2: 0.600\n",
      "########## Fold: 2 ##########\n",
      "MAE: 5.188\n",
      "MSE: 106.660\n",
      "R2: 0.597\n",
      "########## Fold: 3 ##########\n",
      "MAE: 5.146\n",
      "MSE: 97.921\n",
      "R2: 0.605\n",
      "########## Fold: 4 ##########\n",
      "MAE: 5.078\n",
      "MSE: 112.953\n",
      "R2: 0.584\n",
      "########## Fold: 5 ##########\n",
      "MAE: 4.990\n",
      "MSE: 88.761\n",
      "R2: 0.620\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "\n",
    "folds = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "absolute_errors = list()\n",
    "squared_errors = list()\n",
    "r2 = list()\n",
    "\n",
    "for k, (train_index, test_index) in enumerate(folds.split(X, y)):\n",
    "    \n",
    "    print(\"#\"*10 + f\" Fold: {k+1} \" + \"#\"*10)\n",
    "    \n",
    "    X_train_internal, y_train_internal = X.iloc[train_index, :], y.iloc[train_index]\n",
    "    X_test_internal, y_test_internal = X.iloc[test_index, :], y.iloc[test_index]\n",
    "\n",
    "    encoder = CatBoostEncoder()\n",
    "    \n",
    "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    num_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "    cat_pipeline = Pipeline([('encoder', encoder), ('imputer', cat_imputer)])\n",
    "    num_pipeline = Pipeline([('imputer', num_imputer)])\n",
    "\n",
    "    cat_cols = X_train_internal.select_dtypes(include=['object']).columns\n",
    "    num_cols = X_train_internal.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "    X_train_internal[cat_cols] = cat_pipeline.fit_transform(X_train_internal[cat_cols], y_train_internal)\n",
    "    X_train_internal[num_cols] = num_pipeline.fit_transform(X_train_internal[num_cols])\n",
    "\n",
    "    X_test_internal[cat_cols] = cat_pipeline.transform(X_test_internal[cat_cols])\n",
    "    X_test_internal[num_cols] = num_pipeline.transform(X_test_internal[num_cols])\n",
    "\n",
    "    model_XGBoost.fit(X_train_internal, y_train_internal)\n",
    "    y_pred = model_XGBoost.predict(X_test_internal)\n",
    "    r2score = r2_score(y_test_internal, y_pred)\n",
    "    mse = mean_squared_error(y_test_internal, y_pred)\n",
    "    mae = mean_absolute_error(y_test_internal, y_pred)\n",
    "\n",
    "    absolute_errors.append(mae)\n",
    "    squared_errors.append(mse)\n",
    "    r2.append(r2score)\n",
    "\n",
    "    print(f'MAE: {mae:.3f}')\n",
    "    print(f'MSE: {mse:.3f}')\n",
    "    print(f'R2: {r2score:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86730329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Displaying Average of Obtained Metrics : #####\n",
      "Average MAE: 5.089 +/- 0.071\n",
      "Average MSE: 100.963 +/- 8.250\n",
      "Average R2: 0.601 +/- 0.012\n"
     ]
    }
   ],
   "source": [
    "absolute_errors = np.array(absolute_errors)\n",
    "squared_errors = np.array(squared_errors)\n",
    "r2 = np.array(r2)\n",
    "\n",
    "avg_mae = np.mean(absolute_errors)\n",
    "avg_mse = np.mean(squared_errors)\n",
    "avg_r2 = np.mean(r2)\n",
    "\n",
    "std_mae = np.std(absolute_errors)\n",
    "std_mse = np.std(squared_errors)\n",
    "std_r2 = np.std(r2)\n",
    "\n",
    "print(\"#\"*5 + f\" Displaying Average of Obtained Metrics : \" + \"#\"*5)\n",
    "print(f\"Average MAE: {avg_mae:.3f} +/- {std_mae:.3f}\")\n",
    "print(f'Average MSE: {avg_mse:.3f} +/- {std_mse:.3f}')\n",
    "print(f'Average R2: {avg_r2:.3f} +/- {std_r2:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c547e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(X, y, model, k):\n",
    "    folds = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    absolute_errors = list()\n",
    "    squared_errors = list()\n",
    "    r2 = list()\n",
    "\n",
    "    for k, (train_index, test_index) in enumerate(folds.split(X, y)):\n",
    "        \n",
    "        print(\"#\"*10 + f\" Fold: {k+1} \" + \"#\"*10)\n",
    "        \n",
    "        X_train_internal, y_train_internal = X.iloc[train_index, :], y.iloc[train_index]\n",
    "        X_test_internal, y_test_internal = X.iloc[test_index, :], y.iloc[test_index]\n",
    "\n",
    "        encoder = CatBoostEncoder()\n",
    "        \n",
    "        cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "        num_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "        cat_pipeline = Pipeline([('encoder', encoder), ('imputer', cat_imputer)])\n",
    "        num_pipeline = Pipeline([('imputer', num_imputer)])\n",
    "\n",
    "        cat_cols = X_train_internal.select_dtypes(include=['object']).columns\n",
    "        num_cols = X_train_internal.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "        X_train_internal[cat_cols] = cat_pipeline.fit_transform(X_train_internal[cat_cols], y_train_internal)\n",
    "        X_train_internal[num_cols] = num_pipeline.fit_transform(X_train_internal[num_cols])\n",
    "\n",
    "        X_test_internal[cat_cols] = cat_pipeline.transform(X_test_internal[cat_cols])\n",
    "        X_test_internal[num_cols] = num_pipeline.transform(X_test_internal[num_cols])\n",
    "\n",
    "        model.fit(X_train_internal, y_train_internal)\n",
    "        y_pred = model.predict(X_test_internal)\n",
    "\n",
    "        r2score = r2_score(y_test_internal, y_pred)\n",
    "        mse = mean_squared_error(y_test_internal, y_pred)\n",
    "        mae = mean_absolute_error(y_test_internal, y_pred)\n",
    "\n",
    "        absolute_errors.append(mae)\n",
    "        squared_errors.append(mse)\n",
    "        r2.append(r2score)\n",
    "\n",
    "        print(f'MAE: {mae:.3f}')\n",
    "        print(f'MSE: {mse:.3f}')\n",
    "        print(f'R2: {r2score:.3f}')\n",
    "    \n",
    "    absolute_errors = np.array(absolute_errors)\n",
    "    squared_errors = np.array(squared_errors)\n",
    "    r2 = np.array(r2)\n",
    "\n",
    "    avg_mae = np.mean(absolute_errors)\n",
    "    avg_mse = np.mean(squared_errors)\n",
    "    avg_r2 = np.mean(r2)\n",
    "\n",
    "    std_mae = np.std(absolute_errors)\n",
    "    std_mse = np.std(squared_errors)\n",
    "    std_r2 = np.std(r2)\n",
    "\n",
    "    print(\"#\"*5 + f\" Displaying Average of Obtained Metrics : \" + \"#\"*5)\n",
    "    print(f\"Average MAE: {avg_mae:.3f} +/- {std_mae:.3f}\")\n",
    "    print(f'Average MSE: {avg_mse:.3f} +/- {std_mse:.3f}')\n",
    "    print(f'Average R2: {avg_r2:.3f} +/- {std_r2:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b5d1c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_with_mlflow(X, y, model, k, model_name='Model'):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation with MLflow integration.\n",
    "    \"\"\"\n",
    "    folds = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    absolute_errors = list()\n",
    "    squared_errors = list()\n",
    "    r2 = list()\n",
    "    \n",
    "    # Ativar o autolog apropriado com base no modelo\n",
    "    if isinstance(model, XGBRegressor):\n",
    "        mlflow.xgboost.autolog()\n",
    "    elif isinstance(model, LGBMRegressor):\n",
    "        mlflow.lightgbm.autolog()\n",
    "    elif isinstance(model, CatBoostRegressor):\n",
    "        mlflow.catboost.autolog()\n",
    "    elif isinstance(model, DecisionTreeRegressor):\n",
    "        mlflow.sklearn.autolog()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type for autologging.\")\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"{model_name} Cross-Validation\"):\n",
    "        \n",
    "        for k, (train_index, test_index) in enumerate(folds.split(X, y)):\n",
    "            \n",
    "            print(\"#\"*10 + f\" Fold: {k+1} \" + \"#\"*10)\n",
    "            \n",
    "            X_train_internal, y_train_internal = X.iloc[train_index, :], y.iloc[train_index]\n",
    "            X_test_internal, y_test_internal = X.iloc[test_index, :], y.iloc[test_index]\n",
    "\n",
    "            encoder = CatBoostEncoder()\n",
    "            \n",
    "            cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "            num_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "            cat_pipeline = Pipeline([('encoder', encoder), ('imputer', cat_imputer)])\n",
    "            num_pipeline = Pipeline([('imputer', num_imputer)])\n",
    "\n",
    "            cat_cols = X_train_internal.select_dtypes(include=['object']).columns\n",
    "            num_cols = X_train_internal.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "            X_train_internal[cat_cols] = cat_pipeline.fit_transform(X_train_internal[cat_cols], y_train_internal)\n",
    "            X_train_internal[num_cols] = num_pipeline.fit_transform(X_train_internal[num_cols])\n",
    "\n",
    "            X_test_internal[cat_cols] = cat_pipeline.transform(X_test_internal[cat_cols])\n",
    "            X_test_internal[num_cols] = num_pipeline.transform(X_test_internal[num_cols])\n",
    "\n",
    "            model.fit(X_train_internal, y_train_internal)\n",
    "            y_pred = model.predict(X_test_internal)\n",
    "\n",
    "            r2score = r2_score(y_test_internal, y_pred)\n",
    "            mse = mean_squared_error(y_test_internal, y_pred)\n",
    "            mae = mean_absolute_error(y_test_internal, y_pred)\n",
    "\n",
    "            absolute_errors.append(mae)\n",
    "            squared_errors.append(mse)\n",
    "            r2.append(r2score)\n",
    "\n",
    "            print(f'MAE: {mae:.3f}')\n",
    "            print(f'MSE: {mse:.3f}')\n",
    "            print(f'R2: {r2score:.3f}')\n",
    "            \n",
    "            # Registrar m√©tricas do fold no MLflow\n",
    "            mlflow.log_metric(f\"Fold_{k + 1}_MAE\", mae)\n",
    "            mlflow.log_metric(f\"Fold_{k + 1}_MSE\", mse)\n",
    "            mlflow.log_metric(f\"Fold_{k + 1}_R2\", r2score)\n",
    "    \n",
    "        absolute_errors = np.array(absolute_errors)\n",
    "        squared_errors = np.array(squared_errors)\n",
    "        r2 = np.array(r2)\n",
    "\n",
    "        avg_mae = np.mean(absolute_errors)\n",
    "        avg_mse = np.mean(squared_errors)\n",
    "        avg_r2 = np.mean(r2)\n",
    "\n",
    "        std_mae = np.std(absolute_errors)\n",
    "        std_mse = np.std(squared_errors)\n",
    "        std_r2 = np.std(r2)\n",
    "\n",
    "        print(\"#\"*5 + f\" Displaying Average of Obtained Metrics : \" + \"#\"*5)\n",
    "        print(f\"Average MAE: {avg_mae:.3f} +/- {std_mae:.3f}\")\n",
    "        print(f'Average MSE: {avg_mse:.3f} +/- {std_mse:.3f}')\n",
    "        print(f'Average R2: {avg_r2:.3f} +/- {std_r2:.3f}')\n",
    "    \n",
    "        # Registrar m√©tricas m√©dias no MLflow\n",
    "        mlflow.log_metric(\"Average_MAE\", avg_mae)\n",
    "        mlflow.log_metric(\"Average_MSE\", avg_mse)\n",
    "        mlflow.log_metric(\"Average_R2\", avg_r2)\n",
    "\n",
    "        mlflow.log_metric(\"Std_MAE\", std_mae)\n",
    "        mlflow.log_metric(\"Std_MSE\", std_mse)\n",
    "        mlflow.log_metric(\"Std_R2\", std_r2)\n",
    "\n",
    "        # Registrar o modelo no MLflow\n",
    "        mlflow.sklearn.log_model(model, f\"{model_name}_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30b1fbc",
   "metadata": {},
   "source": [
    "### Modelo XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93155869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 5.044\n",
      "MSE: 98.518\n",
      "R2: 0.600\n",
      "########## Fold: 2 ##########\n",
      "MAE: 5.188\n",
      "MSE: 106.660\n",
      "R2: 0.597\n",
      "########## Fold: 3 ##########\n",
      "MAE: 5.146\n",
      "MSE: 97.921\n",
      "R2: 0.605\n",
      "########## Fold: 4 ##########\n",
      "MAE: 5.078\n",
      "MSE: 112.953\n",
      "R2: 0.584\n",
      "########## Fold: 5 ##########\n",
      "MAE: 4.990\n",
      "MSE: 88.761\n",
      "R2: 0.620\n",
      "##### Displaying Average of Obtained Metrics : #####\n",
      "Average MAE: 5.089 +/- 0.071\n",
      "Average MSE: 100.963 +/- 8.250\n",
      "Average R2: 0.601 +/- 0.012\n"
     ]
    }
   ],
   "source": [
    "# Modelo XGBoost\n",
    "cross_validation(X, y, model_XGBoost, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15bec2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/17 18:08:01 WARNING mlflow.utils.autologging_utils: MLflow xgboost autologging is known to be compatible with 1.4.2 <= xgboost <= 2.1.4, but the installed version is 3.0.0. If you encounter errors during autologging, try upgrading / downgrading xgboost to a compatible version, or try upgrading MLflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "MAE: 5.044\n",
      "MSE: 98.518\n",
      "R2: 0.600\n",
      "########## Fold: 2 ##########\n",
      "MAE: 5.188\n",
      "MSE: 106.660\n",
      "R2: 0.597\n",
      "########## Fold: 3 ##########\n",
      "MAE: 5.146\n",
      "MSE: 97.921\n",
      "R2: 0.605\n",
      "########## Fold: 4 ##########\n",
      "MAE: 5.078\n",
      "MSE: 112.953\n",
      "R2: 0.584\n",
      "########## Fold: 5 ##########\n",
      "MAE: 4.990\n",
      "MSE: 88.761\n",
      "R2: 0.620\n",
      "##### Displaying Average of Obtained Metrics : #####\n",
      "Average MAE: 5.089 +/- 0.071\n",
      "Average MSE: 100.963 +/- 8.250\n",
      "Average R2: 0.601 +/- 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/04/17 18:09:08 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run XGBoost Cross-Validation at: http://localhost:5000/#/experiments/535394779431182411/runs/46ff3eb9e8d740ca8259a216758c036e\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/535394779431182411\n"
     ]
    }
   ],
   "source": [
    "cross_validation_with_mlflow(X, y, model_XGBoost, k=5, model_name=\"XGBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3597d6c4",
   "metadata": {},
   "source": [
    "### Modelo Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd96d119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 5.352\n",
      "MSE: 110.291\n",
      "R2: 0.552\n",
      "########## Fold: 2 ##########\n",
      "MAE: 5.520\n",
      "MSE: 117.569\n",
      "R2: 0.555\n",
      "########## Fold: 3 ##########\n",
      "MAE: 5.452\n",
      "MSE: 108.974\n",
      "R2: 0.560\n",
      "########## Fold: 4 ##########\n",
      "MAE: 5.403\n",
      "MSE: 127.166\n",
      "R2: 0.532\n",
      "########## Fold: 5 ##########\n",
      "MAE: 5.321\n",
      "MSE: 100.226\n",
      "R2: 0.571\n",
      "##### Displaying Average of Obtained Metrics : #####\n",
      "Average MAE: 5.410 +/- 0.071\n",
      "Average MSE: 112.845 +/- 9.035\n",
      "Average R2: 0.554 +/- 0.013\n"
     ]
    }
   ],
   "source": [
    "cross_validation(X, y, model_Catboost, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ace8ca99",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'mlflow.catboost' has no attribute 'autolog'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcross_validation_with_mlflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_Catboost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCatBoost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mcross_validation_with_mlflow\u001b[39m\u001b[34m(X, y, model, k, model_name)\u001b[39m\n\u001b[32m     15\u001b[39m     mlflow.lightgbm.autolog()\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, CatBoostRegressor):\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcatboost\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautolog\u001b[49m()\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, DecisionTreeRegressor):\n\u001b[32m     19\u001b[39m     mlflow.sklearn.autolog()\n",
      "\u001b[31mAttributeError\u001b[39m: module 'mlflow.catboost' has no attribute 'autolog'"
     ]
    }
   ],
   "source": [
    "cross_validation_with_mlflow(X, y, model_Catboost, k=5, model_name=\"CatBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db2bc48",
   "metadata": {},
   "source": [
    "### Modelo LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4c1f731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 5.664\n",
      "MSE: 120.227\n",
      "R2: 0.511\n",
      "########## Fold: 2 ##########\n",
      "MAE: 5.827\n",
      "MSE: 128.344\n",
      "R2: 0.515\n",
      "########## Fold: 3 ##########\n",
      "MAE: 5.773\n",
      "MSE: 119.646\n",
      "R2: 0.517\n",
      "########## Fold: 4 ##########\n",
      "MAE: 5.715\n",
      "MSE: 136.550\n",
      "R2: 0.497\n",
      "########## Fold: 5 ##########\n",
      "MAE: 5.612\n",
      "MSE: 108.576\n",
      "R2: 0.535\n",
      "##### Displaying Average of Obtained Metrics : #####\n",
      "Average MAE: 5.718 +/- 0.076\n",
      "Average MSE: 122.669 +/- 9.366\n",
      "Average R2: 0.515 +/- 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/04/17 18:13:28 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run LightGBM Cross-Validation at: http://localhost:5000/#/experiments/535394779431182411/runs/ac8eec71f07c434fab146f0562960587\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/535394779431182411\n"
     ]
    }
   ],
   "source": [
    "# Modelo LightGBM\n",
    "cross_validation_with_mlflow(X, y, model_LightGBM, k=5, model_name=\"LightGBM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5ca5e0",
   "metadata": {},
   "source": [
    "### Modelo Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae495d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "MAE: 5.664\n",
      "MSE: 120.227\n",
      "R2: 0.511\n",
      "########## Fold: 2 ##########\n",
      "MAE: 5.827\n",
      "MSE: 128.344\n",
      "R2: 0.515\n",
      "########## Fold: 3 ##########\n",
      "MAE: 5.773\n",
      "MSE: 119.646\n",
      "R2: 0.517\n",
      "########## Fold: 4 ##########\n",
      "MAE: 5.715\n",
      "MSE: 136.550\n",
      "R2: 0.497\n",
      "########## Fold: 5 ##########\n",
      "MAE: 5.612\n",
      "MSE: 108.576\n",
      "R2: 0.535\n",
      "##### Displaying Average of Obtained Metrics : #####\n",
      "Average MAE: 5.718 +/- 0.076\n",
      "Average MSE: 122.669 +/- 9.366\n",
      "Average R2: 0.515 +/- 0.012\n"
     ]
    }
   ],
   "source": [
    "# Modelo Decision Tree\n",
    "cross_validation(X, y, model_DecisionTree, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19242464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/17 18:17:00 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2025/04/17 18:17:03 WARNING mlflow.sklearn: Training metrics will not be recorded because training labels were not specified. To automatically record training metrics, provide training labels as inputs to the model training function.\n",
      "2025/04/17 18:17:03 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2025/04/17 18:21:09 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during sklearn autologging: The following failures occurred while performing one or more logging operations: [MlflowException('Failed to perform one or more operations on the run with ID 1d24d0a15eb6472c9ffd20a767aab1e6. Failed operations: [MlflowException(\"API request to http://localhost:5000/api/2.0/mlflow/runs/log-batch failed with exception HTTPConnectionPool(host=\\'localhost\\', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/log-batch (Caused by ResponseError(\\'too many 500 error responses\\'))\")]')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 4.370\n",
      "MSE: 88.438\n",
      "R2: 0.641\n",
      "########## Fold: 2 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/17 18:21:16 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2025/04/17 18:21:19 WARNING mlflow.sklearn: Training metrics will not be recorded because training labels were not specified. To automatically record training metrics, provide training labels as inputs to the model training function.\n",
      "2025/04/17 18:21:19 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2025/04/17 18:25:26 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during sklearn autologging: The following failures occurred while performing one or more logging operations: [MlflowException('Failed to perform one or more operations on the run with ID 1d24d0a15eb6472c9ffd20a767aab1e6. Failed operations: [MlflowException(\"API request to http://localhost:5000/api/2.0/mlflow/runs/log-batch failed with exception HTTPConnectionPool(host=\\'localhost\\', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/log-batch (Caused by ResponseError(\\'too many 500 error responses\\'))\")]')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 4.468\n",
      "MSE: 92.460\n",
      "R2: 0.650\n",
      "########## Fold: 3 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/17 18:25:32 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2025/04/17 18:25:35 WARNING mlflow.sklearn: Training metrics will not be recorded because training labels were not specified. To automatically record training metrics, provide training labels as inputs to the model training function.\n",
      "2025/04/17 18:25:35 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2025/04/17 18:29:42 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during sklearn autologging: The following failures occurred while performing one or more logging operations: [MlflowException('Failed to perform one or more operations on the run with ID 1d24d0a15eb6472c9ffd20a767aab1e6. Failed operations: [MlflowException(\"API request to http://localhost:5000/api/2.0/mlflow/runs/log-batch failed with exception HTTPConnectionPool(host=\\'localhost\\', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/log-batch (Caused by ResponseError(\\'too many 500 error responses\\'))\")]')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 4.393\n",
      "MSE: 85.635\n",
      "R2: 0.654\n",
      "########## Fold: 4 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/17 18:29:49 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2025/04/17 18:29:52 WARNING mlflow.sklearn: Training metrics will not be recorded because training labels were not specified. To automatically record training metrics, provide training labels as inputs to the model training function.\n",
      "2025/04/17 18:29:52 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2025/04/17 18:33:58 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during sklearn autologging: The following failures occurred while performing one or more logging operations: [MlflowException('Failed to perform one or more operations on the run with ID 1d24d0a15eb6472c9ffd20a767aab1e6. Failed operations: [MlflowException(\"API request to http://localhost:5000/api/2.0/mlflow/runs/log-batch failed with exception HTTPConnectionPool(host=\\'localhost\\', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/log-batch (Caused by ResponseError(\\'too many 500 error responses\\'))\")]')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 4.371\n",
      "MSE: 100.245\n",
      "R2: 0.631\n",
      "########## Fold: 5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/17 18:34:05 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2025/04/17 18:34:08 WARNING mlflow.sklearn: Training metrics will not be recorded because training labels were not specified. To automatically record training metrics, provide training labels as inputs to the model training function.\n",
      "2025/04/17 18:34:08 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2025/04/17 18:38:14 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during sklearn autologging: The following failures occurred while performing one or more logging operations: [MlflowException('Failed to perform one or more operations on the run with ID 1d24d0a15eb6472c9ffd20a767aab1e6. Failed operations: [MlflowException(\"API request to http://localhost:5000/api/2.0/mlflow/runs/log-batch failed with exception HTTPConnectionPool(host=\\'localhost\\', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/log-batch (Caused by ResponseError(\\'too many 500 error responses\\'))\")]')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 4.331\n",
      "MSE: 85.238\n",
      "R2: 0.635\n",
      "##### Displaying Average of Obtained Metrics : #####\n",
      "Average MAE: 4.387 +/- 0.045\n",
      "Average MSE: 90.404 +/- 5.556\n",
      "Average R2: 0.642 +/- 0.009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/04/17 18:38:22 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run Decision Tree Cross-Validation at: http://localhost:5000/#/experiments/535394779431182411/runs/1d24d0a15eb6472c9ffd20a767aab1e6\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/535394779431182411\n"
     ]
    }
   ],
   "source": [
    "cross_validation_with_mlflow(X, y, model_DecisionTree, k=5, model_name=\"Decision Tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58121e7e",
   "metadata": {},
   "source": [
    "## 9. Fine-Tuning\n",
    "We use Optuna to perform hyperparameter optimization for the XGBoost model to improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a24d8dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInit signature:\u001b[39m\n",
      "XGBRegressor(\n",
      "    *,\n",
      "    objective: Union[str, xgboost.sklearn._SklObjWProto, Callable[[Any, Any], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = \u001b[33m'reg:squarederror'\u001b[39m,\n",
      "    **kwargs: Any,\n",
      ") -> \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mDocstring:\u001b[39m     \n",
      "Implementation of the scikit-learn API for XGBoost regression.\n",
      "See :doc:`/python/sklearn_estimator` for more information.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "\n",
      "    n_estimators : typing.Optional[int]\n",
      "        Number of gradient boosted trees.  Equivalent to number of boosting\n",
      "        rounds.\n",
      "\n",
      "    max_depth :  typing.Optional[int]\n",
      "\n",
      "        Maximum tree depth for base learners.\n",
      "\n",
      "    max_leaves : typing.Optional[int]\n",
      "\n",
      "        Maximum number of leaves; 0 indicates no limit.\n",
      "\n",
      "    max_bin : typing.Optional[int]\n",
      "\n",
      "        If using histogram-based algorithm, maximum number of bins per feature\n",
      "\n",
      "    grow_policy : typing.Optional[str]\n",
      "\n",
      "        Tree growing policy.\n",
      "\n",
      "        - depthwise: Favors splitting at nodes closest to the node,\n",
      "        - lossguide: Favors splitting at nodes with highest loss change.\n",
      "\n",
      "    learning_rate : typing.Optional[float]\n",
      "\n",
      "        Boosting learning rate (xgb's \"eta\")\n",
      "\n",
      "    verbosity : typing.Optional[int]\n",
      "\n",
      "        The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "\n",
      "    objective : typing.Union[str, xgboost.sklearn._SklObjWProto, typing.Callable[[typing.Any, typing.Any], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
      "\n",
      "        Specify the learning task and the corresponding learning objective or a custom\n",
      "        objective function to be used.\n",
      "\n",
      "        For custom objective, see :doc:`/tutorials/custom_metric_obj` and\n",
      "        :ref:`custom-obj-metric` for more information, along with the end note for\n",
      "        function signatures.\n",
      "\n",
      "    booster: typing.Optional[str]\n",
      "\n",
      "        Specify which booster to use: ``gbtree``, ``gblinear`` or ``dart``.\n",
      "\n",
      "    tree_method : typing.Optional[str]\n",
      "\n",
      "        Specify which tree method to use.  Default to auto.  If this parameter is set to\n",
      "        default, XGBoost will choose the most conservative option available.  It's\n",
      "        recommended to study this option from the parameters document :doc:`tree method\n",
      "        </treemethod>`\n",
      "\n",
      "    n_jobs : typing.Optional[int]\n",
      "\n",
      "        Number of parallel threads used to run xgboost.  When used with other\n",
      "        Scikit-Learn algorithms like grid search, you may choose which algorithm to\n",
      "        parallelize and balance the threads.  Creating thread contention will\n",
      "        significantly slow down both algorithms.\n",
      "\n",
      "    gamma : typing.Optional[float]\n",
      "\n",
      "        (min_split_loss) Minimum loss reduction required to make a further partition on\n",
      "        a leaf node of the tree.\n",
      "\n",
      "    min_child_weight : typing.Optional[float]\n",
      "\n",
      "        Minimum sum of instance weight(hessian) needed in a child.\n",
      "\n",
      "    max_delta_step : typing.Optional[float]\n",
      "\n",
      "        Maximum delta step we allow each tree's weight estimation to be.\n",
      "\n",
      "    subsample : typing.Optional[float]\n",
      "\n",
      "        Subsample ratio of the training instance.\n",
      "\n",
      "    sampling_method : typing.Optional[str]\n",
      "\n",
      "        Sampling method. Used only by the GPU version of ``hist`` tree method.\n",
      "\n",
      "        - ``uniform``: Select random training instances uniformly.\n",
      "        - ``gradient_based``: Select random training instances with higher probability\n",
      "            when the gradient and hessian are larger. (cf. CatBoost)\n",
      "\n",
      "    colsample_bytree : typing.Optional[float]\n",
      "\n",
      "        Subsample ratio of columns when constructing each tree.\n",
      "\n",
      "    colsample_bylevel : typing.Optional[float]\n",
      "\n",
      "        Subsample ratio of columns for each level.\n",
      "\n",
      "    colsample_bynode : typing.Optional[float]\n",
      "\n",
      "        Subsample ratio of columns for each split.\n",
      "\n",
      "    reg_alpha : typing.Optional[float]\n",
      "\n",
      "        L1 regularization term on weights (xgb's alpha).\n",
      "\n",
      "    reg_lambda : typing.Optional[float]\n",
      "\n",
      "        L2 regularization term on weights (xgb's lambda).\n",
      "\n",
      "    scale_pos_weight : typing.Optional[float]\n",
      "        Balancing of positive and negative weights.\n",
      "\n",
      "    base_score : typing.Optional[float]\n",
      "\n",
      "        The initial prediction score of all instances, global bias.\n",
      "\n",
      "    random_state : typing.Union[numpy.random.mtrand.RandomState, numpy.random._generator.Generator, int, NoneType]\n",
      "\n",
      "        Random number seed.\n",
      "\n",
      "        .. note::\n",
      "\n",
      "           Using gblinear booster with shotgun updater is nondeterministic as\n",
      "           it uses Hogwild algorithm.\n",
      "\n",
      "    missing : float\n",
      "\n",
      "        Value in the data which needs to be present as a missing value. Default to\n",
      "        :py:data:`numpy.nan`.\n",
      "\n",
      "    num_parallel_tree: typing.Optional[int]\n",
      "\n",
      "        Used for boosting random forest.\n",
      "\n",
      "    monotone_constraints : typing.Union[typing.Dict[str, int], str, NoneType]\n",
      "\n",
      "        Constraint of variable monotonicity.  See :doc:`tutorial </tutorials/monotonic>`\n",
      "        for more information.\n",
      "\n",
      "    interaction_constraints : typing.Union[str, typing.List[typing.Tuple[str]], NoneType]\n",
      "\n",
      "        Constraints for interaction representing permitted interactions.  The\n",
      "        constraints must be specified in the form of a nested list, e.g. ``[[0, 1], [2,\n",
      "        3, 4]]``, where each inner list is a group of indices of features that are\n",
      "        allowed to interact with each other.  See :doc:`tutorial\n",
      "        </tutorials/feature_interaction_constraint>` for more information\n",
      "\n",
      "    importance_type: typing.Optional[str]\n",
      "\n",
      "        The feature importance type for the feature_importances\\_ property:\n",
      "\n",
      "        * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
      "          \"total_cover\".\n",
      "        * For linear model, only \"weight\" is defined and it's the normalized\n",
      "          coefficients without bias.\n",
      "\n",
      "    device : typing.Optional[str]\n",
      "\n",
      "        .. versionadded:: 2.0.0\n",
      "\n",
      "        Device ordinal, available options are `cpu`, `cuda`, and `gpu`.\n",
      "\n",
      "    validate_parameters : typing.Optional[bool]\n",
      "\n",
      "        Give warnings for unknown parameter.\n",
      "\n",
      "    enable_categorical : bool\n",
      "\n",
      "        See the same parameter of :py:class:`DMatrix` for details.\n",
      "\n",
      "    feature_types : typing.Optional[typing.Sequence[str]]\n",
      "\n",
      "        .. versionadded:: 1.7.0\n",
      "\n",
      "        Used for specifying feature types without constructing a dataframe. See\n",
      "        :py:class:`DMatrix` for details.\n",
      "\n",
      "    feature_weights : Optional[ArrayLike]\n",
      "\n",
      "        Weight for each feature, defines the probability of each feature being selected\n",
      "        when colsample is being used.  All values must be greater than 0, otherwise a\n",
      "        `ValueError` is thrown.\n",
      "\n",
      "    max_cat_to_onehot : Optional[int]\n",
      "\n",
      "        .. versionadded:: 1.6.0\n",
      "\n",
      "        .. note:: This parameter is experimental\n",
      "\n",
      "        A threshold for deciding whether XGBoost should use one-hot encoding based split\n",
      "        for categorical data.  When number of categories is lesser than the threshold\n",
      "        then one-hot encoding is chosen, otherwise the categories will be partitioned\n",
      "        into children nodes. Also, `enable_categorical` needs to be set to have\n",
      "        categorical feature support. See :doc:`Categorical Data\n",
      "        </tutorials/categorical>` and :ref:`cat-param` for details.\n",
      "\n",
      "    max_cat_threshold : typing.Optional[int]\n",
      "\n",
      "        .. versionadded:: 1.7.0\n",
      "\n",
      "        .. note:: This parameter is experimental\n",
      "\n",
      "        Maximum number of categories considered for each split. Used only by\n",
      "        partition-based splits for preventing over-fitting. Also, `enable_categorical`\n",
      "        needs to be set to have categorical feature support. See :doc:`Categorical Data\n",
      "        </tutorials/categorical>` and :ref:`cat-param` for details.\n",
      "\n",
      "    multi_strategy : typing.Optional[str]\n",
      "\n",
      "        .. versionadded:: 2.0.0\n",
      "\n",
      "        .. note:: This parameter is working-in-progress.\n",
      "\n",
      "        The strategy used for training multi-target models, including multi-target\n",
      "        regression and multi-class classification. See :doc:`/tutorials/multioutput` for\n",
      "        more information.\n",
      "\n",
      "        - ``one_output_per_tree``: One model for each target.\n",
      "        - ``multi_output_tree``:  Use multi-target trees.\n",
      "\n",
      "    eval_metric : typing.Union[str, typing.List[str], typing.Callable, NoneType]\n",
      "\n",
      "        .. versionadded:: 1.6.0\n",
      "\n",
      "        Metric used for monitoring the training result and early stopping.  It can be a\n",
      "        string or list of strings as names of predefined metric in XGBoost (See\n",
      "        :doc:`/parameter`), one of the metrics in :py:mod:`sklearn.metrics`, or any\n",
      "        other user defined metric that looks like `sklearn.metrics`.\n",
      "\n",
      "        If custom objective is also provided, then custom metric should implement the\n",
      "        corresponding reverse link function.\n",
      "\n",
      "        Unlike the `scoring` parameter commonly used in scikit-learn, when a callable\n",
      "        object is provided, it's assumed to be a cost function and by default XGBoost\n",
      "        will minimize the result during early stopping.\n",
      "\n",
      "        For advanced usage on Early stopping like directly choosing to maximize instead\n",
      "        of minimize, see :py:obj:`xgboost.callback.EarlyStopping`.\n",
      "\n",
      "        See :doc:`/tutorials/custom_metric_obj` and :ref:`custom-obj-metric` for more\n",
      "        information.\n",
      "\n",
      "        .. code-block:: python\n",
      "\n",
      "            from sklearn.datasets import load_diabetes\n",
      "            from sklearn.metrics import mean_absolute_error\n",
      "            X, y = load_diabetes(return_X_y=True)\n",
      "            reg = xgb.XGBRegressor(\n",
      "                tree_method=\"hist\",\n",
      "                eval_metric=mean_absolute_error,\n",
      "            )\n",
      "            reg.fit(X, y, eval_set=[(X, y)])\n",
      "\n",
      "    early_stopping_rounds : typing.Optional[int]\n",
      "\n",
      "        .. versionadded:: 1.6.0\n",
      "\n",
      "        - Activates early stopping. Validation metric needs to improve at least once in\n",
      "          every **early_stopping_rounds** round(s) to continue training.  Requires at\n",
      "          least one item in **eval_set** in :py:meth:`fit`.\n",
      "\n",
      "        - If early stopping occurs, the model will have two additional attributes:\n",
      "          :py:attr:`best_score` and :py:attr:`best_iteration`. These are used by the\n",
      "          :py:meth:`predict` and :py:meth:`apply` methods to determine the optimal\n",
      "          number of trees during inference. If users want to access the full model\n",
      "          (including trees built after early stopping), they can specify the\n",
      "          `iteration_range` in these inference methods. In addition, other utilities\n",
      "          like model plotting can also use the entire model.\n",
      "\n",
      "        - If you prefer to discard the trees after `best_iteration`, consider using the\n",
      "          callback function :py:class:`xgboost.callback.EarlyStopping`.\n",
      "\n",
      "        - If there's more than one item in **eval_set**, the last entry will be used for\n",
      "          early stopping.  If there's more than one metric in **eval_metric**, the last\n",
      "          metric will be used for early stopping.\n",
      "\n",
      "    callbacks : typing.Optional[typing.List[xgboost.callback.TrainingCallback]]\n",
      "\n",
      "        List of callback functions that are applied at end of each iteration.\n",
      "        It is possible to use predefined callbacks by using\n",
      "        :ref:`Callback API <callback_api>`.\n",
      "\n",
      "        .. note::\n",
      "\n",
      "           States in callback are not preserved during training, which means callback\n",
      "           objects can not be reused for multiple training sessions without\n",
      "           reinitialization or deepcopy.\n",
      "\n",
      "        .. code-block:: python\n",
      "\n",
      "            for params in parameters_grid:\n",
      "                # be sure to (re)initialize the callbacks before each run\n",
      "                callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]\n",
      "                reg = xgboost.XGBRegressor(**params, callbacks=callbacks)\n",
      "                reg.fit(X, y)\n",
      "\n",
      "    kwargs : typing.Optional[typing.Any]\n",
      "\n",
      "        Keyword arguments for XGBoost Booster object.  Full documentation of parameters\n",
      "        can be found :doc:`here </parameter>`.\n",
      "        Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
      "        dict simultaneously will result in a TypeError.\n",
      "\n",
      "        .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "\n",
      "            \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
      "            that parameters passed via this argument will interact properly\n",
      "            with scikit-learn.\n",
      "\n",
      "        .. note::  Custom objective function\n",
      "\n",
      "            A custom objective function can be provided for the ``objective``\n",
      "            parameter. In this case, it should have the signature ``objective(y_true,\n",
      "            y_pred) -> [grad, hess]`` or ``objective(y_true, y_pred, *, sample_weight)\n",
      "            -> [grad, hess]``:\n",
      "\n",
      "            y_true: array_like of shape [n_samples]\n",
      "                The target values\n",
      "            y_pred: array_like of shape [n_samples]\n",
      "                The predicted values\n",
      "            sample_weight :\n",
      "                Optional sample weights.\n",
      "\n",
      "            grad: array_like of shape [n_samples]\n",
      "                The value of the gradient for each sample point.\n",
      "            hess: array_like of shape [n_samples]\n",
      "                The value of the second derivative for each sample point\n",
      "\n",
      "            Note that, if the custom objective produces negative values for\n",
      "            the Hessian, these will be clipped. If the objective is non-convex,\n",
      "            one might also consider using the expected Hessian (Fisher\n",
      "            information) instead.\n",
      "\u001b[31mFile:\u001b[39m           /opt/conda/envs/olist-freight/lib/python3.13/site-packages/xgboost/sklearn.py\n",
      "\u001b[31mType:\u001b[39m           type\n",
      "\u001b[31mSubclasses:\u001b[39m     XGBRFRegressor"
     ]
    }
   ],
   "source": [
    "?XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b05d0a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 17:48:39,203] A new study created in memory with name: no-name-2766b846-7baa-4183-ab42-8eb032c77802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "########## Fold: 2 ##########\n",
      "########## Fold: 3 ##########\n",
      "########## Fold: 4 ##########\n",
      "########## Fold: 5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 17:49:26,556] Trial 0 finished with value: 100.96271617963595 and parameters: {'learning_rate': 0.0014385451992090926, 'max_depth': 6, 'subsample': 1.0, 'colsample_bytree': 0.8, 'min_child_weight': 1}. Best is trial 0 with value: 100.96271617963595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "########## Fold: 2 ##########\n",
      "########## Fold: 3 ##########\n",
      "########## Fold: 4 ##########\n",
      "########## Fold: 5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 17:50:12,368] Trial 1 finished with value: 100.96271617963595 and parameters: {'learning_rate': 0.019413921538283543, 'max_depth': 3, 'subsample': 0.6, 'colsample_bytree': 0.9, 'min_child_weight': 1}. Best is trial 0 with value: 100.96271617963595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "########## Fold: 2 ##########\n",
      "########## Fold: 3 ##########\n",
      "########## Fold: 4 ##########\n",
      "########## Fold: 5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 17:50:58,873] Trial 2 finished with value: 100.96271617963595 and parameters: {'learning_rate': 0.017738500853710506, 'max_depth': 6, 'subsample': 0.6, 'colsample_bytree': 0.9, 'min_child_weight': 5}. Best is trial 0 with value: 100.96271617963595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "########## Fold: 2 ##########\n",
      "########## Fold: 3 ##########\n",
      "########## Fold: 4 ##########\n",
      "########## Fold: 5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 17:51:44,811] Trial 3 finished with value: 100.96271617963595 and parameters: {'learning_rate': 0.047404271450566984, 'max_depth': 2, 'subsample': 0.5, 'colsample_bytree': 0.6, 'min_child_weight': 7}. Best is trial 0 with value: 100.96271617963595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "########## Fold: 2 ##########\n",
      "########## Fold: 3 ##########\n",
      "########## Fold: 4 ##########\n",
      "########## Fold: 5 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 17:52:30,385] Trial 4 finished with value: 100.96271617963595 and parameters: {'learning_rate': 0.05920617625648391, 'max_depth': 9, 'subsample': 0.6, 'colsample_bytree': 0.8, 'min_child_weight': 9}. Best is trial 0 with value: 100.96271617963595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "########## Fold: 2 ##########\n",
      "########## Fold: 3 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-04-17 17:53:00,159] Trial 5 failed with parameters: {'learning_rate': 0.003384630250152554, 'max_depth': 8, 'subsample': 0.8, 'colsample_bytree': 1.0, 'min_child_weight': 6} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/olist-freight/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_46436/2029287172.py\", line 40, in fine_tuning\n",
      "    y_pred = model_XGBoost.predict(X_test_internal)\n",
      "  File \"/opt/conda/envs/olist-freight/lib/python3.13/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/opt/conda/envs/olist-freight/lib/python3.13/site-packages/xgboost/sklearn.py\", line 1327, in predict\n",
      "    predts = self.get_booster().inplace_predict(\n",
      "        data=X,\n",
      "    ...<4 lines>...\n",
      "        validate_features=validate_features,\n",
      "    )\n",
      "  File \"/opt/conda/envs/olist-freight/lib/python3.13/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/opt/conda/envs/olist-freight/lib/python3.13/site-packages/xgboost/core.py\", line 2700, in inplace_predict\n",
      "    _LIB.XGBoosterPredictFromColumnar(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self.handle,\n",
      "        ^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        ctypes.byref(preds),\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "KeyboardInterrupt\n",
      "[W 2025-04-17 17:53:00,163] Trial 5 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m avg_mse\n\u001b[32m     64\u001b[39m study = opt.create_study(direction=\u001b[33m'\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfine_tuning\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/olist-freight/lib/python3.13/site-packages/optuna/study/study.py:475\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    374\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    375\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    383\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    385\u001b[39m \n\u001b[32m    386\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    473\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/olist-freight/lib/python3.13/site-packages/optuna/study/_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/olist-freight/lib/python3.13/site-packages/optuna/study/_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/olist-freight/lib/python3.13/site-packages/optuna/study/_optimize.py:248\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    244\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    245\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    246\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    247\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/olist-freight/lib/python3.13/site-packages/optuna/study/_optimize.py:197\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    199\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    200\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mfine_tuning\u001b[39m\u001b[34m(trial, k)\u001b[39m\n\u001b[32m     37\u001b[39m X_test_internal[num_cols] = num_pipeline.transform(X_test_internal[num_cols])\n\u001b[32m     39\u001b[39m model_XGBoost.fit(X_train_internal, y_train_internal)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m y_pred = \u001b[43mmodel_XGBoost\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_internal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m r2score = r2_score(y_test_internal, y_pred)\n\u001b[32m     42\u001b[39m mse = mean_squared_error(y_test_internal, y_pred)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/olist-freight/lib/python3.13/site-packages/xgboost/core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/olist-freight/lib/python3.13/site-packages/xgboost/sklearn.py:1327\u001b[39m, in \u001b[36mXGBModel.predict\u001b[39m\u001b[34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[39m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._can_use_inplace_predict():\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1327\u001b[39m         predts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minplace_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m=\u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmargin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1335\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_alike(predts):\n\u001b[32m   1336\u001b[39m             cp = import_cupy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/olist-freight/lib/python3.13/site-packages/xgboost/core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/olist-freight/lib/python3.13/site-packages/xgboost/core.py:2700\u001b[39m, in \u001b[36mBooster.inplace_predict\u001b[39m\u001b[34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[39m\n\u001b[32m   2697\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _prediction_output(shape, dims, preds, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   2698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (ArrowTransformed, PandasTransformed)):\n\u001b[32m   2699\u001b[39m     _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2700\u001b[39m         \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterPredictFromColumnar\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2701\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2702\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray_interface\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2703\u001b[39m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2704\u001b[39m \u001b[43m            \u001b[49m\u001b[43mp_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2705\u001b[39m \u001b[43m            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2706\u001b[39m \u001b[43m            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdims\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2707\u001b[39m \u001b[43m            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2708\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2709\u001b[39m     )\n\u001b[32m   2710\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _prediction_output(shape, dims, preds, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   2711\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, scipy.sparse.csr_matrix):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def fine_tuning(trial, k=5):\n",
    "    # tuning\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True)\n",
    "    max_depth = trial.suggest_int('max_depth', 1, 10)\n",
    "    subsample = trial.suggest_float('subsample', 0.5, 1, step=0.1)\n",
    "    colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1, step=0.1)\n",
    "    min_child_weight = trial.suggest_int('min_child_weight', 1, 10)\n",
    "\n",
    "    folds = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    absolute_errors = list()\n",
    "    squared_errors = list()\n",
    "    r2 = list()\n",
    "\n",
    "    for k, (train_index, test_index) in enumerate(folds.split(X, y)):\n",
    "        \n",
    "        print(\"#\"*10 + f\" Fold: {k+1} \" + \"#\"*10)\n",
    "        \n",
    "        X_train_internal, y_train_internal = X.iloc[train_index, :], y.iloc[train_index]\n",
    "        X_test_internal, y_test_internal = X.iloc[test_index, :], y.iloc[test_index]\n",
    "\n",
    "        encoder = CatBoostEncoder()\n",
    "        \n",
    "        cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "        num_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "        cat_pipeline = Pipeline([('encoder', encoder), ('imputer', cat_imputer)])\n",
    "        num_pipeline = Pipeline([('imputer', num_imputer)])\n",
    "\n",
    "        cat_cols = X_train_internal.select_dtypes(include=['object']).columns\n",
    "        num_cols = X_train_internal.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "        X_train_internal[cat_cols] = cat_pipeline.fit_transform(X_train_internal[cat_cols], y_train_internal)\n",
    "        X_train_internal[num_cols] = num_pipeline.fit_transform(X_train_internal[num_cols])\n",
    "\n",
    "        X_test_internal[cat_cols] = cat_pipeline.transform(X_test_internal[cat_cols])\n",
    "        X_test_internal[num_cols] = num_pipeline.transform(X_test_internal[num_cols])\n",
    "\n",
    "        model_XGBoost.fit(X_train_internal, y_train_internal)\n",
    "        y_pred = model_XGBoost.predict(X_test_internal)\n",
    "        r2score = r2_score(y_test_internal, y_pred)\n",
    "        mse = mean_squared_error(y_test_internal, y_pred)\n",
    "        mae = mean_absolute_error(y_test_internal, y_pred)\n",
    "\n",
    "        absolute_errors.append(mae)\n",
    "        squared_errors.append(mse)\n",
    "        r2.append(r2score)\n",
    "\n",
    "    \n",
    "    absolute_errors = np.array(absolute_errors)\n",
    "    squared_errors = np.array(squared_errors)\n",
    "    r2 = np.array(r2)\n",
    "\n",
    "    avg_mae = np.mean(absolute_errors)\n",
    "    avg_mse = np.mean(squared_errors)\n",
    "    avg_r2 = np.mean(r2)\n",
    "\n",
    "    std_mae = np.std(absolute_errors)\n",
    "    std_mse = np.std(squared_errors)\n",
    "    std_r2 = np.std(r2)\n",
    "\n",
    "    return avg_mse\n",
    "\n",
    "study = opt.create_study(direction='minimize')\n",
    "study.optimize(fine_tuning, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01ee732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 18:57:36,786] A new study created in memory with name: no-name-fd47ce24-d8c8-4de4-ace3-c39eb2146a51\n",
      "2025/04/17 18:57:36 WARNING mlflow.utils.autologging_utils: MLflow xgboost autologging is known to be compatible with 1.4.2 <= xgboost <= 2.1.4, but the installed version is 3.0.0. If you encounter errors during autologging, try upgrading / downgrading xgboost to a compatible version, or try upgrading MLflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/17 18:57:37 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2025/04/17 18:57:40 WARNING mlflow.sklearn: Training metrics will not be recorded because training labels were not specified. To automatically record training metrics, provide training labels as inputs to the model training function.\n",
      "2025/04/17 18:57:40 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n"
     ]
    }
   ],
   "source": [
    "def fine_tuning(trial, k=5):\n",
    "    # tuning\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True)\n",
    "    max_depth = trial.suggest_int('max_depth', 1, 10)\n",
    "    subsample = trial.suggest_float('subsample', 0.5, 1, step=0.1)\n",
    "    colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1, step=0.1)\n",
    "    min_child_weight = trial.suggest_int('min_child_weight', 1, 10)\n",
    "\n",
    "    folds = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    absolute_errors = list()\n",
    "    squared_errors = list()\n",
    "    r2 = list()\n",
    "\n",
    "    mlflow.xgboost.autolog()\n",
    "\n",
    "    # Iniciar um run no MLflow\n",
    "    with mlflow.start_run(run_name=\"Optuna Fine-Tuning\"):\n",
    "        \n",
    "        # Registrar os hiperpar√¢metros no MLflow\n",
    "        mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "        mlflow.log_param(\"max_depth\", max_depth)\n",
    "        mlflow.log_param(\"subsample\", subsample)\n",
    "        mlflow.log_param(\"colsample_bytree\", colsample_bytree)\n",
    "        mlflow.log_param(\"min_child_weight\", min_child_weight)\n",
    "    \n",
    "        for k, (train_index, test_index) in enumerate(folds.split(X, y)):\n",
    "            \n",
    "            print(\"#\"*10 + f\" Fold: {k+1} \" + \"#\"*10)\n",
    "            \n",
    "            X_train_internal, y_train_internal = X.iloc[train_index, :], y.iloc[train_index]\n",
    "            X_test_internal, y_test_internal = X.iloc[test_index, :], y.iloc[test_index]\n",
    "\n",
    "            encoder = CatBoostEncoder()\n",
    "            \n",
    "            cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "            num_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "            cat_pipeline = Pipeline([('encoder', encoder), ('imputer', cat_imputer)])\n",
    "            num_pipeline = Pipeline([('imputer', num_imputer)])\n",
    "\n",
    "            cat_cols = X_train_internal.select_dtypes(include=['object']).columns\n",
    "            num_cols = X_train_internal.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "            X_train_internal[cat_cols] = cat_pipeline.fit_transform(X_train_internal[cat_cols], y_train_internal)\n",
    "            X_train_internal[num_cols] = num_pipeline.fit_transform(X_train_internal[num_cols])\n",
    "\n",
    "            X_test_internal[cat_cols] = cat_pipeline.transform(X_test_internal[cat_cols])\n",
    "            X_test_internal[num_cols] = num_pipeline.transform(X_test_internal[num_cols])\n",
    "\n",
    "            model_XGBoost.fit(X_train_internal, y_train_internal)\n",
    "            y_pred = model_XGBoost.predict(X_test_internal)\n",
    "            \n",
    "            r2score = r2_score(y_test_internal, y_pred)\n",
    "            mse = mean_squared_error(y_test_internal, y_pred)\n",
    "            mae = mean_absolute_error(y_test_internal, y_pred)\n",
    "\n",
    "            absolute_errors.append(mae)\n",
    "            squared_errors.append(mse)\n",
    "            r2.append(r2score)\n",
    "            \n",
    "            # Registrar m√©tricas do fold no MLflow\n",
    "            mlflow.log_metric(f\"Fold_{k + 1}_MAE\", mae)\n",
    "            mlflow.log_metric(f\"Fold_{k + 1}_MSE\", mse)\n",
    "            mlflow.log_metric(f\"Fold_{k + 1}_R2\", r2score)\n",
    "\n",
    "        \n",
    "        absolute_errors = np.array(absolute_errors)\n",
    "        squared_errors = np.array(squared_errors)\n",
    "        r2 = np.array(r2)\n",
    "\n",
    "        avg_mae = np.mean(absolute_errors)\n",
    "        avg_mse = np.mean(squared_errors)\n",
    "        avg_r2 = np.mean(r2)\n",
    "\n",
    "        std_mae = np.std(absolute_errors)\n",
    "        std_mse = np.std(squared_errors)\n",
    "        std_r2 = np.std(r2)\n",
    "        \n",
    "        # Registrar m√©tricas m√©dias no MLflow\n",
    "        mlflow.log_metric(\"Average_MAE\", avg_mae)\n",
    "        mlflow.log_metric(\"Average_MSE\", avg_mse)\n",
    "        mlflow.log_metric(\"Average_R2\", avg_r2)\n",
    "\n",
    "        mlflow.log_metric(\"Std_MAE\", std_mae)\n",
    "        mlflow.log_metric(\"Std_MSE\", std_mse)\n",
    "        mlflow.log_metric(\"Std_R2\", std_r2)\n",
    "        \n",
    "        # Registrar o modelo no MLflow\n",
    "        mlflow.xgboost.log_model(model_XGBoost, \"XGBoost_Model\")\n",
    "\n",
    "    return avg_mse\n",
    "\n",
    "study = opt.create_study(direction='minimize')\n",
    "study.optimize(fine_tuning, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641a9285",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'learning_rate': 0.008335068262687652, 'max_depth': 5, 'subsample': 0.6, 'colsample_bytree': 0.9, 'min_child_weight': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26068523",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_XGBoost = XGBRegressor(n_estimators=1000, n_jobs=-1, random_state=0, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d3672a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "MAE: 3.785\n",
      "MSE: 64.887\n",
      "R2: 0.736\n",
      "########## Fold: 2 ##########\n",
      "MAE: 3.925\n",
      "MSE: 67.987\n",
      "R2: 0.743\n",
      "########## Fold: 3 ##########\n",
      "MAE: 3.843\n",
      "MSE: 62.634\n",
      "R2: 0.747\n",
      "########## Fold: 4 ##########\n",
      "MAE: 3.821\n",
      "MSE: 72.702\n",
      "R2: 0.732\n",
      "########## Fold: 5 ##########\n",
      "MAE: 3.774\n",
      "MSE: 55.732\n",
      "R2: 0.761\n",
      "##### Displaying Average of Obtained Metrics : #####\n",
      "Average MAE: 3.829 +/- 0.054\n",
      "Average MSE: 64.788 +/- 5.648\n",
      "Average R2: 0.744 +/- 0.010\n"
     ]
    }
   ],
   "source": [
    "cross_validation_with_mlflow(X, y, model_XGBoost, k=5, model_name='XGBoost Fine-Tuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb63e23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def feature_engineering(X_train, X_test):\n",
    "\n",
    "#     X_train['volume'] = X_train['product_length_cm'] * X_train['product_height_cm'] * X_train['product_width_cm']\n",
    "#     X_test['volume'] = X_test['product_length_cm'] * X_test['product_height_cm'] * X_test['product_width_cm']\n",
    "\n",
    "#     # other important features\n",
    "\n",
    "#     return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7024062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cross_validation(X, y, model, k):\n",
    "#     folds = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "#     absolute_errors = list()\n",
    "#     squared_errors = list()\n",
    "#     r2 = list()\n",
    "\n",
    "#     for k, (train_index, test_index) in enumerate(folds.split(X, y)):\n",
    "        \n",
    "#         print(\"#\"*10 + f\" Fold: {k+1} \" + \"#\"*10)\n",
    "        \n",
    "#         X_train_internal, y_train_internal = X.iloc[train_index, :], y.iloc[train_index]\n",
    "#         X_test_internal, y_test_internal = X.iloc[test_index, :], y.iloc[test_index]\n",
    "\n",
    "#         encoder = CatBoostEncoder()\n",
    "        \n",
    "#         cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "#         num_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "#         cat_pipeline = Pipeline([('encoder', encoder), ('imputer', cat_imputer)])\n",
    "#         num_pipeline = Pipeline([('imputer', num_imputer)])\n",
    "\n",
    "#         X_train_internal, X_test_internal = feature_engineering(X_train_internal, X_test_internal)\n",
    "\n",
    "#         cat_cols = X_train_internal.select_dtypes(include=['object']).columns\n",
    "#         num_cols = X_train_internal.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "#         X_train_internal[cat_cols] = cat_pipeline.fit_transform(X_train_internal[cat_cols], y_train_internal)\n",
    "#         X_train_internal[num_cols] = num_pipeline.fit_transform(X_train_internal[num_cols])\n",
    "\n",
    "#         X_test_internal[cat_cols] = cat_pipeline.transform(X_test_internal[cat_cols])\n",
    "#         X_test_internal[num_cols] = num_pipeline.transform(X_test_internal[num_cols])\n",
    "\n",
    "#         model.fit(X_train_internal, y_train_internal)\n",
    "#         y_pred = model.predict(X_test_internal)\n",
    "#         r2score = r2_score(y_test_internal, y_pred)\n",
    "#         mse = mean_squared_error(y_test_internal, y_pred)\n",
    "#         mae = mean_absolute_error(y_test_internal, y_pred)\n",
    "\n",
    "#         absolute_errors.append(mae)\n",
    "#         squared_errors.append(mse)\n",
    "#         r2.append(r2score)\n",
    "\n",
    "#         print(f'MAE: {mae:.3f}')\n",
    "#         print(f'MSE: {mse:.3f}')\n",
    "#         print(f'R2: {r2score:.3f}')\n",
    "    \n",
    "#     absolute_errors = np.array(absolute_errors)\n",
    "#     squared_errors = np.array(squared_errors)\n",
    "#     r2 = np.array(r2)\n",
    "\n",
    "#     avg_mae = np.mean(absolute_errors)\n",
    "#     avg_mse = np.mean(squared_errors)\n",
    "#     avg_r2 = np.mean(r2)\n",
    "\n",
    "#     std_mae = np.std(absolute_errors)\n",
    "#     std_mse = np.std(squared_errors)\n",
    "#     std_r2 = np.std(r2)\n",
    "\n",
    "#     print(\"#\"*5 + f\" Displaying Average of Obtained Metrics : \" + \"#\"*5)\n",
    "#     print(f\"Average MAE: {avg_mae:.3f} +/- {std_mae:.3f}\")\n",
    "#     print(f'Average MSE: {avg_mse:.3f} +/- {std_mse:.3f}')\n",
    "#     print(f'Average R2: {avg_r2:.3f} +/- {std_r2:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74725c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Fold: 1 ##########\n",
      "MAE: 5.664\n",
      "MSE: 120.227\n",
      "R2: 0.511\n",
      "########## Fold: 2 ##########\n",
      "MAE: 5.827\n",
      "MSE: 128.344\n",
      "R2: 0.515\n",
      "########## Fold: 3 ##########\n",
      "MAE: 5.773\n",
      "MSE: 119.646\n",
      "R2: 0.517\n",
      "########## Fold: 4 ##########\n",
      "MAE: 5.715\n",
      "MSE: 136.550\n",
      "R2: 0.497\n",
      "########## Fold: 5 ##########\n",
      "MAE: 5.612\n",
      "MSE: 108.576\n",
      "R2: 0.535\n",
      "##### Displaying Average of Obtained Metrics : #####\n",
      "Average MAE: 5.718 +/- 0.076\n",
      "Average MSE: 122.669 +/- 9.366\n",
      "Average R2: 0.515 +/- 0.012\n"
     ]
    }
   ],
   "source": [
    "# cross_validation(X, y, model_LightGBM, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2b7c11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olist-freight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
